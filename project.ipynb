{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5b1edd",
   "metadata": {},
   "source": [
    "# Jupyter Snake\n",
    "\n",
    "---\n",
    "\n",
    "In this project our goal is to beat the game of snake, applying multiple RL techniques in order to teach an agent how to play the game.\\\n",
    "The project is divided in 3 parts:\n",
    "1. Developement of the Environment\n",
    "2. Implementation of the Algorithms\n",
    "3. Learning and evaluating phase of the Algorithms\n",
    "\n",
    "This whole project is developed as final project for the \"Reinforcement Learning\" course (2024-2025).\n",
    "\n",
    "Authors : *Bredariol Francesco, Savorgnan Enrico, Tic Ruben*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27a3a695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T16:17:04.294653Z",
     "start_time": "2025-06-09T16:17:02.414322Z"
    }
   },
   "outputs": [],
   "source": [
    "from algorithms import *\n",
    "from eligibility_traces import *\n",
    "from epsilon_scheduler import * \n",
    "from snake_environment import *\n",
    "from states_bracket import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32296b",
   "metadata": {},
   "source": [
    "## PART 1\n",
    "---\n",
    "*Environment*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e4414",
   "metadata": {},
   "source": [
    "### **The Game**\n",
    "\n",
    "For who who doesn't know Snake is a not just a game but a genre of action video games.\\\n",
    "It was born in 1976 competitive arcade video game Blockade, where the goal was to survive longer than others players while collecting the most possible food.\\\n",
    "In this game you control the head of a snake on grid world and you aim to eat some food in order to become bigger. The big difficulty here is that if you hit your tail (this is the only common rule for all snake variant) you die.\\\n",
    "There are multiple version of the game and some of them are really weird (where teleportation can occour or some food can actually make you die).\\\n",
    "We took in account the most basic version, where:\n",
    "\n",
    "1. The world is a discrete grid world of size $n\\times m$\n",
    "2. There is always only one food (apple) on the grid world, and when you it changes position\n",
    "3. There are no periodic boundary conditions, meaning that if you hit a wall you die\n",
    "\n",
    "The rest is just as described as in the introduction to the game.\\\n",
    "Little side note is that this version is inspired by the \"Snake Byte\" published by Sirius Software in 1982.\n",
    "\n",
    "### **The Implementation**\n",
    "\n",
    "Thanks to Gymnasium and PyGame the implementation of this simple version of the game is pretty straightforward.\\\n",
    "Developed completely in Python in the file \"snake_environment.py\", this implementation follows the Gym protocol, defining:\n",
    "\n",
    "1. Step\n",
    "2. Reset\n",
    "3. Render, which use PyGame\n",
    "4. Close\n",
    "\n",
    "All others functions are private and only used inside the class, for the exception of \"get_possible_action\" which is useful for our purpose.\n",
    "\n",
    "One important thing is that we actually defined a maximum number of step inside the environment to prevent infinte loop driven by bad policies while training. This is a parameter for the __init__ with default value 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ecb7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "env = SnakeEnv(render_mode=\"human\")\n",
    "\n",
    "done, keep = False, True\n",
    "\n",
    "state, _ = env.reset()\n",
    "action = 0\n",
    "\n",
    "while not done and keep:\n",
    "    action = random.choice(env.get_possible_actions(action))\n",
    "    state, reward, done, trunc, inf = env.step(action)\n",
    "    keep = env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413723b9",
   "metadata": {},
   "source": [
    "### **The Dimensionality Problem**\n",
    "\n",
    "Once the environment is defined, one can think about how big the space of all possible configuration is.\\\n",
    "Well, doing some (pretty bad but although valid) approximation, considering a state as the matrix representation of the grid with 0 (empty cell), 1 (apple), 2(head) and 3 (tail), the dimension of all possible configuration ends up being something like this:\n",
    "$$\n",
    "    |S| = (n\\times m)(n\\times m)2^{(n\\times m)}\n",
    "$$\n",
    "This should describe all the possible positions of the apple, all the possible position of the head and all possible configuration on the grid of the tails (now this is the big approximation, since the tail configuration is not independent from the head position). Anyway, even if this is an approximation one can simply add the \"blocks\" on the grid world (static cells that kill, if touched, the snake) and the dimension should exactly being that big.\n",
    "\n",
    "Now this is not a simple thing to deal with while learning. Solution? Bidding (or bracketing, how we actually call it). Now on this soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858975fc",
   "metadata": {},
   "source": [
    "## PART 2\n",
    "---\n",
    "*Algorithms*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483518f2659e757e",
   "metadata": {},
   "source": [
    "## **BIDDING (or Bracketing)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea080f",
   "metadata": {},
   "source": [
    "As shown before the Snake game has a huge state dimension.\\\n",
    "Since a look up table of that dimension has no logic to exist (I don't even think our computers can store something like that) and since it is pretty impossible just to see one example for each pair state-action in an entire life time, something had to come in mind.\\\n",
    "Thanks to a lesson we learnt about bidding (that we called bracketing for the entire project, shame on Francesco) and decided to try it out.\\\n",
    "Bidding essentially is just a supervised technique (it is the human that codifies how it works) that agglomerates similar states together, in order to reduce the dimensionality of the problem.\\\n",
    "A very stupid example could be the following: each state just randomly be labelled as 0 or 1. And the agents now will not see the entire state representation, but only the label you gave it. Now this example is stupid because, using this random strategy, you end up with no knowledge at all. BUT, look at what happened at your state dimension: it fell down from whatever it was to 2! Pretty neat, uh?\n",
    "\n",
    "We discussed together end decided to try a lot of different bidding techniques, and we end up discovering the big tradeoff in this field: **information against dimension**.\\\n",
    "Minimal state bidding are easy to implement, but if they are too small it is not ensured that they will bring enough information to actually learn to the agent. On the other hand, if you give too many information to the agent, you will end up again with a too big state dimension to deal with.\n",
    "\n",
    "Another important aspect of bidding is that you actually \"completely\" lose the transition function below your system. It is true that you can develop a new transition function on a new space, the \"bidding space\", but it is not easy and it is not ensured to be relevant.\n",
    "\n",
    "Now we will quickly see all the biddings we implemented. We defined in the states_bracket.py a super class (StateBracket) which implement the protocol for all the bidding techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87984876fcb07286",
   "metadata": {},
   "source": [
    "### **FOOD INFORMATION ONLY**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a573a8",
   "metadata": {},
   "source": [
    "Our first approach was to give only information about the position of the apple wrt the head of the snake. Two variants of this idea came up:\n",
    "\n",
    "1. Food Relative Position \n",
    "2. Food Direction\n",
    "\n",
    "Both of the techniques are pretty straightforward to implement, and their computation is very fast as well. However, the snake loses information about its tail, the walls and the boundaries of the cell.\n",
    "\n",
    "\n",
    "##### **Food Relative Position**\n",
    "\n",
    "Once you get the apple position $(a_y, a_x)$ and the head position $(h_y, h_x)$ you just return as state the tuple $(a_y - h_y, a_x - h_x)$. This approach reduces the states' dimension up to $2m \\times 2n$\n",
    "\n",
    "##### **Food Direction**\n",
    "\n",
    "Once you get the apple position $(a_y, a_x)$ and the head position $(h_y, h_x)$ you just return as state the tuple $(a_y < h_y, a_y > h_x, a_x < h_x, a_x > h_x)$.\\\n",
    "Now, while the first one is straightforward, this a little more subtle. This tells you whether your head is above, below, to the left, or to the right of the food. \\\n",
    "This is a very minimal information: the states are condensed into just $8$ bins.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22adea2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, -1)\n",
      "(0, 0, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "state = np.array([\n",
    "    [0, 0, 0], \n",
    "    [0, 0, 1], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "frp = FoodRelativePositionBracket()\n",
    "fd = FoodDirectionBracket()\n",
    "print(frp.bracket(state))\n",
    "print(fd.bracket(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a32a1",
   "metadata": {},
   "source": [
    "\n",
    "These bidding techniques combines the things we have just seen plus information relative to the neighborhood of the head.\\\n",
    "Neighborhood could be both of Von Neumann type or Moore type. The radius is a parameter for this bidding. Notice that the greater the radius, the greater the total state dimension. The information within the neighborhood are expressed in the form of 0 and 1. 0 if a cell is free and 1 if a cell is occupied by the tail.\n",
    "\n",
    "For further details on the implementation we suggest to directly read the code, which is full commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d316e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, -1, 0, 0, 0, 1, 1)\n",
      "(0, 0, 1, 0, 0, 0, 0, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "state = np.array([\n",
    "    [0, 0, 0], \n",
    "    [0, 0, 1], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "npfrp = NeighPlusFoodRelativePositionBracket(radius=1)\n",
    "npfd = NeighPlusFoodDirectionBracket(radius=1)\n",
    "print(npfrp.bracket(state))\n",
    "print(npfd.bracket(state))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **ONLY NEIGHBORHOOD INFORMATION**",
   "id": "10f3df511eb745b8"
  },
  {
   "cell_type": "markdown",
   "id": "23404406",
   "metadata": {},
   "source": [
    "An interesting experiment was the definition of the bidding containing only information relative to the neighborhood of the head (adding the 2 value for the apple in this case).\\\n",
    "The idea was that, using this type of bidding, the agent could have learnt to search being really careful, but probably with no knowledge on its own position it is impossible for it to learn a valid strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21e4114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "state = np.array([\n",
    "    [0, 0, 0], \n",
    "    [0, 0, 1], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "n = NeighborhoodBracket()\n",
    "print(n.bracket(state))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **FOOD AND NEIGHBORHOOD AND TAIL INFORMATION**",
   "id": "7418591460032e21"
  },
  {
   "cell_type": "markdown",
   "id": "5c1c5d7e",
   "metadata": {},
   "source": [
    "The last type of bidding explored combines all the informations described until now plus a little information about the tail.\\\n",
    "The relative information about the tail is its length, and should help the agent learn to be a little more careful when the tail gets longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4515a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, -1, 0, 0, 0, 1, 1)\n",
      "(0, 0, 0, 1, 0, 0, 0, 0, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "state = np.array([\n",
    "    [0, 0, 0], \n",
    "    [0, 0, 1], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "tpnpfrp = NeighPlusFoodRelativePositionPlusTailBracket()\n",
    "tpnpfd = NeighPlusFoodDirectionPlusTailBracket()\n",
    "print(tpnpfrp.bracket(state))\n",
    "print(tpnpfd.bracket(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2fdac6f7b385f",
   "metadata": {},
   "source": [
    "## ALGORITHMS"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Why not MDP**",
   "id": "807422ccd42da3e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "How we have seen in the latest section, the only way to obtain a dealable state dimension is using the binning. \\\n",
    "The problem is that, using binning, we lose the transition function below our MDP. Indeed, being in the same state, taking the same action, ending into the same state, could possibly lead to different rewards.\n",
    "To give an example, consider the two situations below, where we marked with a $O$ the head of the snake, with $0$ its body and with $X$ the food (consider the snake without tail):\n",
    "\n",
    "\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " & 0 & 0 \\\\\n",
    "\\hline\n",
    " & O & 0 \\\\\n",
    "\\hline\n",
    "X &  &  \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " &  &  \\\\\n",
    "\\hline\n",
    " & O & 0 \\\\\n",
    "\\hline\n",
    "X &  & 0 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "Such situations are bidded into the same condensed-state $(-1, -1)$. However, the action \"UP\" will lead in the first example to a negative reward because of the hit of the snake's tail, while  in the second case, to a zero reward.\n",
    "\n",
    "This erases the capability to retrieve good results solving the problem as an MDP via policy iteration.\n",
    "\n",
    "TO ARGUMENT A BIT MORE\n"
   ],
   "id": "3c9e58165593d034"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Our Choices**",
   "id": "a4bcf80a5c80f3f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "In the end we decided to develop 5 different **Model-free algorithms** (with also some variants of them) in order to take familiarity with the whole RL framework. These are the algorithms we implemented:\n",
    "\n",
    "1. Montecarlo \n",
    "2. SARSA\n",
    "3. QLearning\n",
    "4. DDQL\n",
    "5. Policy Gradient\n",
    "\n",
    "We firstly implemented a super class that defines a protocol for all the algorithms and provides useful function such as \"get_action_epsilon_greedy\" and so on.\\\n",
    "Useful method of the class are the save and upload methods, which can let to store the results obtained as a pickle dictionary in order to retrieve it later.\\\n",
    "In addiction the utils.py contains a lot of useful function used to deal with the default_dict, a structure we used to store the QValues look up table (for the algorithms that require it). Since we used a lot of bidding we used default dict to possibly deal with no-fixed state dimension.\n",
    "\n",
    "Before diving in our actual implementation, let's briefly remind some key concepts that will be used.\n",
    "\n",
    "*Credit for the key concepts to [this tutorial](https://medium.com/@hsinhungw/intro-to-reinforcement-learning-monte-carlo-to-policy-gradient-1c7ede4eed6e)*"
   ],
   "id": "6eb6c21303a49c91"
  },
  {
   "cell_type": "markdown",
   "id": "45bcd296",
   "metadata": {},
   "source": [
    "### **Policy-based vs Value-based**\n",
    "\n",
    "Most of our algorithms are value-based but policy gradient is in fact policy-based. Let's define the difference between these two classes.\n",
    "\n",
    "1. **Policy-based methods**: The agent learns the policy directly.\n",
    "2. **Value-based methods**: The agent learns a value function that gives the expected return of being in a given state or performing a given action in a given state. The policy can then be derived from the learned value function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102cbf66",
   "metadata": {},
   "source": [
    "### **Off-policy / On-policy**\n",
    "\n",
    "In RL, the agent generates experience by interacting with the environment under a certain policy and then learns the policy from those experiences. Given this in mind, algorithms can use different approaches to the policy. Two main classes exist, let's break them down.\n",
    "\n",
    "\n",
    "1. **On-policy methods**: The agent attempts to learn the policy that is also used to generate the experience.\n",
    "2. **Off-policy methods**: The agent learns a policy that is different from the one used to generate experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21365df2f240c3",
   "metadata": {},
   "source": [
    "### **Q-Value Learning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da42c7",
   "metadata": {},
   "source": [
    "Our goal is to find the optimal policy given only the experience of an environment. Again, the experience consists in the trajectories we perform as we explore the environment\n",
    "\n",
    "$\\tau_i = (S^i_0, A^i_0, R^i_1, S^i_1, A^i_1, R^i_1, \\dots S^i_{T^i})$\n",
    "\n",
    "\n",
    "To do this, it will be more convenient to consider, instead of the _state_-function $V_{\\pi} (s) $, the _state/action_-function $Q_{\\pi}(s,a)$. It is defined as:\n",
    "\n",
    "$$\n",
    "Q_{\\pi}(s,a) = \\mathbb{E}_\\pi\\bigg[ \\sum_{t=0}^\\infty \\gamma^t \\, R_{t+1} \\, \\, \\Big| \\, \\, S_0 = s, A_0 = a \\bigg] \\ .\n",
    "$$\n",
    "\n",
    "**Definition 1** _\"The expectation discounted cumulative sum of all future rewards when starting from state $s$, acting with action $a$ and then always following the policy $\\pi$\"_\n",
    "\n",
    "or equivalently as:\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) = \\mathbb{E}_{\\pi}\\bigg[ R_1 + \\gamma V_\\pi(S_1) \\,\\, \\Big| \\,\\, S_0 = s, A_0 = a\\bigg] \\ .\n",
    "$$\n",
    "\n",
    "**Definition 2** _\"The expectation value of the immediate reward plus the discounted $V$-value of the following state $S_1$, when starting from state $s$, acting with action $a$ and then always following the policy $\\pi$.\"_\n",
    "\n",
    "or again as:\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) = \\mathbb{E}_{\\pi}\\bigg[ R_1 + \\gamma  \\, \\sum_{a'} \\, Q_\\pi(S_1, a') \\, \\pi(a' | S_1) \\,\\, \\Big| \\,\\, S_0 = s, A_0 = a\\bigg] \\ .\n",
    "$$\n",
    "**Definition 3** _\"The expectation value of the immediate reward plus the discounted $Q$-value of the following state $S_1$ and all possible actions $a'$ weighted by the probability of taking that action ($\\pi(a' | S_1)$), when starting from state $s$, acting with action $a$ and then always following the policy $\\pi$.\"_\n",
    "\n",
    "*Credit to the 4th tutorial for the definition*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc529469b589858",
   "metadata": {},
   "source": [
    "### **Montecarlo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ebd2f1",
   "metadata": {},
   "source": [
    "\n",
    "Monte Carlo methods are ways of estimating value functions through experiences (sample sequence of states, actions, and rewards). Recall that the value of a state is the expected return (expected cumulative future discounted reward) starting from that state. One way to estimate the expected return is simply to average the returns observed after visits to that state. As more returns are observed, by the law of large numbers, the average should converge to the expected return. This idea underlies all Monte Carlo methods.\n",
    "\n",
    "Our first algorithm developed was the **on-policy Montecarlo**. It works as follows:\n",
    "\n",
    "1. On-policy MC uses an **ε-soft policy**. A soft policy refers to any policy that has a small, but finite, probability of selecting any possible action, ensuring exploration of alternative actions.\n",
    "2. After each episode, the **observed returns** are used to learn the q-value function, and then the policy is improved based on the learned value function for all the states visited in the episode.\n",
    "\n",
    "This means that we use the first definition. This definition has no bias but an high variance.\n",
    "\n",
    "![onpolicymc](./images/on_policy_mc_correct.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669a7b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300/400 : epsilon 0.3\n",
      "\n",
      "\n",
      "Learning finished\n",
      "\n",
      "\n",
      "Episode 100 : Average performance -18.35\n",
      "Episode 200 : Average performance -18.735\n",
      "Episode 300 : Average performance -15.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=100)\n",
    "mc = Montecarlo(action_space=4, gamma=0.90, lr_v=0.01)\n",
    "eps = ConstantEpsilonDecay(0.3)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "mc.learning(env = env, epsilon_schedule= eps, n_episodes=400, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "mc.play(env=env, bracketer=bracketer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc4903f2f2b661b",
   "metadata": {},
   "source": [
    "### **SARSA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb80610",
   "metadata": {},
   "source": [
    "SARSA is a fascinating on-policy method which use state action and reward (State Action Reward State Action) in order to achieve its goals. There are many variants of SARSA, and all of them are different in terms of how they update the Q-Value table. ```(ENRICO) Quali versioni? -> Expected Sarsa, Sarsa, lambda Sarsa? Forse sono solo expected sarsa e sarsa```\n",
    "\n",
    "We implemented both a SARSA(0) version, where the Q-values are updated each step, and a SARSA(λ). \\\n",
    "The second option, despite being a bit slower, is a more general and flexible algorithm which leaves the possibility to see more than one action at the time, so to have a more accurate update of the Q-values. In this version of SARSA we implemented a \"backward approach\" thanks to eligibility traces.\n",
    "\n",
    "The (common) pseudocode for SARSA(0) is here below:\n",
    "\n",
    "![sarsa](./images/sarsa.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "705855f5aec89f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T16:23:33.225550Z",
     "start_time": "2025-06-09T16:23:25.698634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400/500 : epsilon 0.3\n",
      "\n",
      "\n",
      "Learning finished\n",
      "\n",
      "\n",
      "Episode 100 : Average performance -18.675\n",
      "Episode 200 : Average performance -19.7\n",
      "Episode 300 : Average performance -19.55\n",
      "Episode 400 : Average performance -17.275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-20.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=100)\n",
    "sarsa = SARSA(action_space=4, gamma=0.90, lr_v=0.01)\n",
    "eps = ConstantEpsilonDecay(0.3)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "sarsa.learning(env = env, epsilon_schedule= eps, n_episodes=500, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "sarsa.play(env=env, bracketer=bracketer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c140674ddcc063d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T16:21:04.295633Z",
     "start_time": "2025-06-09T16:20:59.850542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400/500 : epsilon 0.3\n",
      "\n",
      "\n",
      "Learning finished\n",
      "\n",
      "\n",
      "Episode 100 : Average performance -21.33\n",
      "Episode 200 : Average performance -20.685\n",
      "Episode 300 : Average performance -19.06\n",
      "Episode 400 : Average performance -17.68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-7.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=100)\n",
    "sarsal = SARSALambda(action_space=4, gamma=0.90, lr_v=0.01, lambda_value=0.1)\n",
    "eps = ConstantEpsilonDecay(0.3)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "sarsal.learning(env = env, epsilon_schedule= eps, n_episodes=500, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "sarsal.play(env=env, bracketer=bracketer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcdbbd",
   "metadata": {},
   "source": [
    "### **QLearning**\n",
    "\n",
    "QLearning is the first off-policy algorithm we see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "489d34f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19900/20000 : epsilon 0.2\n",
      "\n",
      "\n",
      "Learning finished\n",
      "\n",
      "\n",
      "Episode 100 : Average performance -26.82\n",
      "Episode 200 : Average performance -26.29\n",
      "Episode 300 : Average performance -23.28\n",
      "Episode 400 : Average performance -21.2\n",
      "Episode 500 : Average performance -22.625\n",
      "Episode 600 : Average performance -20.36\n",
      "Episode 700 : Average performance -22.225\n",
      "Episode 800 : Average performance -18.7\n",
      "Episode 900 : Average performance -20.82\n",
      "Episode 1000 : Average performance -18.785\n",
      "Episode 1100 : Average performance -17.22\n",
      "Episode 1200 : Average performance -17.065\n",
      "Episode 1300 : Average performance -16.36\n",
      "Episode 1400 : Average performance -17.47\n",
      "Episode 1500 : Average performance -15.205\n",
      "Episode 1600 : Average performance -13.87\n",
      "Episode 1700 : Average performance -16.05\n",
      "Episode 1800 : Average performance -13.88\n",
      "Episode 1900 : Average performance -13.25\n",
      "Episode 2000 : Average performance -14.7\n",
      "Episode 2100 : Average performance -15.09\n",
      "Episode 2200 : Average performance -13.67\n",
      "Episode 2300 : Average performance -12.33\n",
      "Episode 2400 : Average performance -15.26\n",
      "Episode 2500 : Average performance -13.355\n",
      "Episode 2600 : Average performance -15.675\n",
      "Episode 2700 : Average performance -15.635\n",
      "Episode 2800 : Average performance -13.95\n",
      "Episode 2900 : Average performance -14.37\n",
      "Episode 3000 : Average performance -13.99\n",
      "Episode 3100 : Average performance -14.81\n",
      "Episode 3200 : Average performance -15.645\n",
      "Episode 3300 : Average performance -12.89\n",
      "Episode 3400 : Average performance -15.17\n",
      "Episode 3500 : Average performance -14.855\n",
      "Episode 3600 : Average performance -14.29\n",
      "Episode 3700 : Average performance -15.08\n",
      "Episode 3800 : Average performance -13.86\n",
      "Episode 3900 : Average performance -15.73\n",
      "Episode 4000 : Average performance -14.815\n",
      "Episode 4100 : Average performance -14.8\n",
      "Episode 4200 : Average performance -14.085\n",
      "Episode 4300 : Average performance -13.255\n",
      "Episode 4400 : Average performance -15.16\n",
      "Episode 4500 : Average performance -15.0\n",
      "Episode 4600 : Average performance -13.99\n",
      "Episode 4700 : Average performance -14.92\n",
      "Episode 4800 : Average performance -13.705\n",
      "Episode 4900 : Average performance -14.565\n",
      "Episode 5000 : Average performance -14.45\n",
      "Episode 5100 : Average performance -14.125\n",
      "Episode 5200 : Average performance -13.87\n",
      "Episode 5300 : Average performance -14.0\n",
      "Episode 5400 : Average performance -14.445\n",
      "Episode 5500 : Average performance -13.84\n",
      "Episode 5600 : Average performance -14.12\n",
      "Episode 5700 : Average performance -11.64\n",
      "Episode 5800 : Average performance -13.57\n",
      "Episode 5900 : Average performance -13.525\n",
      "Episode 6000 : Average performance -13.84\n",
      "Episode 6100 : Average performance -13.09\n",
      "Episode 6200 : Average performance -12.65\n",
      "Episode 6300 : Average performance -12.005\n",
      "Episode 6400 : Average performance -15.605\n",
      "Episode 6500 : Average performance -13.155\n",
      "Episode 6600 : Average performance -14.435\n",
      "Episode 6700 : Average performance -14.04\n",
      "Episode 6800 : Average performance -13.83\n",
      "Episode 6900 : Average performance -15.285\n",
      "Episode 7000 : Average performance -14.515\n",
      "Episode 7100 : Average performance -13.025\n",
      "Episode 7200 : Average performance -13.98\n",
      "Episode 7300 : Average performance -14.23\n",
      "Episode 7400 : Average performance -13.94\n",
      "Episode 7500 : Average performance -13.18\n",
      "Episode 7600 : Average performance -13.575\n",
      "Episode 7700 : Average performance -12.795\n",
      "Episode 7800 : Average performance -13.985\n",
      "Episode 7900 : Average performance -14.445\n",
      "Episode 8000 : Average performance -14.72\n",
      "Episode 8100 : Average performance -13.25\n",
      "Episode 8200 : Average performance -13.18\n",
      "Episode 8300 : Average performance -13.48\n",
      "Episode 8400 : Average performance -13.775\n",
      "Episode 8500 : Average performance -13.815\n",
      "Episode 8600 : Average performance -13.35\n",
      "Episode 8700 : Average performance -14.065\n",
      "Episode 8800 : Average performance -13.56\n",
      "Episode 8900 : Average performance -15.89\n",
      "Episode 9000 : Average performance -15.285\n",
      "Episode 9100 : Average performance -14.37\n",
      "Episode 9200 : Average performance -14.055\n",
      "Episode 9300 : Average performance -15.6\n",
      "Episode 9400 : Average performance -14.625\n",
      "Episode 9500 : Average performance -14.84\n",
      "Episode 9600 : Average performance -13.805\n",
      "Episode 9700 : Average performance -13.385\n",
      "Episode 9800 : Average performance -15.255\n",
      "Episode 9900 : Average performance -12.775\n",
      "Episode 10000 : Average performance -16.28\n",
      "Episode 10100 : Average performance -13.015\n",
      "Episode 10200 : Average performance -14.255\n",
      "Episode 10300 : Average performance -13.99\n",
      "Episode 10400 : Average performance -14.76\n",
      "Episode 10500 : Average performance -14.21\n",
      "Episode 10600 : Average performance -14.275\n",
      "Episode 10700 : Average performance -15.51\n",
      "Episode 10800 : Average performance -15.555\n",
      "Episode 10900 : Average performance -13.955\n",
      "Episode 11000 : Average performance -13.99\n",
      "Episode 11100 : Average performance -12.515\n",
      "Episode 11200 : Average performance -15.645\n",
      "Episode 11300 : Average performance -13.27\n",
      "Episode 11400 : Average performance -15.25\n",
      "Episode 11500 : Average performance -14.265\n",
      "Episode 11600 : Average performance -14.755\n",
      "Episode 11700 : Average performance -13.785\n",
      "Episode 11800 : Average performance -14.26\n",
      "Episode 11900 : Average performance -14.615\n",
      "Episode 12000 : Average performance -14.27\n",
      "Episode 12100 : Average performance -13.17\n",
      "Episode 12200 : Average performance -13.935\n",
      "Episode 12300 : Average performance -13.98\n",
      "Episode 12400 : Average performance -13.63\n",
      "Episode 12500 : Average performance -14.785\n",
      "Episode 12600 : Average performance -12.215\n",
      "Episode 12700 : Average performance -14.635\n",
      "Episode 12800 : Average performance -13.29\n",
      "Episode 12900 : Average performance -13.95\n",
      "Episode 13000 : Average performance -14.995\n",
      "Episode 13100 : Average performance -13.59\n",
      "Episode 13200 : Average performance -12.225\n",
      "Episode 13300 : Average performance -14.645\n",
      "Episode 13400 : Average performance -14.14\n",
      "Episode 13500 : Average performance -12.33\n",
      "Episode 13600 : Average performance -14.05\n",
      "Episode 13700 : Average performance -14.17\n",
      "Episode 13800 : Average performance -12.75\n",
      "Episode 13900 : Average performance -13.45\n",
      "Episode 14000 : Average performance -15.19\n",
      "Episode 14100 : Average performance -12.725\n",
      "Episode 14200 : Average performance -14.425\n",
      "Episode 14300 : Average performance -13.005\n",
      "Episode 14400 : Average performance -14.265\n",
      "Episode 14500 : Average performance -13.315\n",
      "Episode 14600 : Average performance -14.53\n",
      "Episode 14700 : Average performance -12.605\n",
      "Episode 14800 : Average performance -13.525\n",
      "Episode 14900 : Average performance -14.51\n",
      "Episode 15000 : Average performance -14.0\n",
      "Episode 15100 : Average performance -14.435\n",
      "Episode 15200 : Average performance -13.7\n",
      "Episode 15300 : Average performance -12.955\n",
      "Episode 15400 : Average performance -14.315\n",
      "Episode 15500 : Average performance -13.845\n",
      "Episode 15600 : Average performance -14.27\n",
      "Episode 15700 : Average performance -14.51\n",
      "Episode 15800 : Average performance -14.305\n",
      "Episode 15900 : Average performance -13.825\n",
      "Episode 16000 : Average performance -14.16\n",
      "Episode 16100 : Average performance -14.845\n",
      "Episode 16200 : Average performance -13.41\n",
      "Episode 16300 : Average performance -12.475\n",
      "Episode 16400 : Average performance -14.06\n",
      "Episode 16500 : Average performance -13.405\n",
      "Episode 16600 : Average performance -13.395\n",
      "Episode 16700 : Average performance -13.74\n",
      "Episode 16800 : Average performance -12.885\n",
      "Episode 16900 : Average performance -13.305\n",
      "Episode 17000 : Average performance -14.095\n",
      "Episode 17100 : Average performance -13.7\n",
      "Episode 17200 : Average performance -13.415\n",
      "Episode 17300 : Average performance -14.495\n",
      "Episode 17400 : Average performance -13.64\n",
      "Episode 17500 : Average performance -13.38\n",
      "Episode 17600 : Average performance -13.71\n",
      "Episode 17700 : Average performance -13.72\n",
      "Episode 17800 : Average performance -13.465\n",
      "Episode 17900 : Average performance -12.63\n",
      "Episode 18000 : Average performance -12.72\n",
      "Episode 18100 : Average performance -13.855\n",
      "Episode 18200 : Average performance -13.455\n",
      "Episode 18300 : Average performance -12.91\n",
      "Episode 18400 : Average performance -13.72\n",
      "Episode 18500 : Average performance -13.125\n",
      "Episode 18600 : Average performance -13.79\n",
      "Episode 18700 : Average performance -12.47\n",
      "Episode 18800 : Average performance -14.15\n",
      "Episode 18900 : Average performance -14.13\n",
      "Episode 19000 : Average performance -13.895\n",
      "Episode 19100 : Average performance -12.955\n",
      "Episode 19200 : Average performance -14.34\n",
      "Episode 19300 : Average performance -14.265\n",
      "Episode 19400 : Average performance -13.605\n",
      "Episode 19500 : Average performance -14.15\n",
      "Episode 19600 : Average performance -13.78\n",
      "Episode 19700 : Average performance -13.485\n",
      "Episode 19800 : Average performance -14.61\n",
      "Episode 19900 : Average performance -15.56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=1000)\n",
    "ql = QLearning(action_space=4, gamma=0.90, lr_v=0.01)\n",
    "eps = LinearEpsilonDecay(1, 0.999, 0.2)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "ql.learning(env = env, epsilon_schedule= eps, n_episodes=20000, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "ql.play(env=env, bracketer=bracketer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41d96a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ql.play(env=env, bracketer=bracketer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0021c49bf663843",
   "metadata": {},
   "source": [
    "### **Double Deep Q Learning (DDQL)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe29e80016fc9fcc",
   "metadata": {},
   "source": [
    "This algorithm is a variant of the Q-Learning algorithm, which is an off-policy method.\n",
    "\n",
    "It is basically a combination of Q-Learning and Deep Q-Networks (DQN), used to estimate the Q-values.\n",
    "The main idea behind DDQL is to use two separate neural networks for the estimation, which helps to reduce the overestimation bias that can occur in Deep Q-learning and in Q-learning in general.\n",
    "Indeed, in common Q-Learning, the use of the $\\max$ operator may lead to inflated Q-values. This consequently leads to suboptimal policies, especially in complex environments. \\\n",
    "\n",
    "DDQL addresses the problem by exploiting a **Online neural network** for choosing the best action given a certain state, and using a **Target neural network** for computing the estimated Q-values given the couple (state, action).\n",
    "The target network is updated less frequently than the online one, so the two are basically decoupled, and their predictions are independent. This helps to stabilize the learning process and reduce the overestimation bias.\n",
    "\n",
    "![ddql](./images/ddql.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096aa114ce8a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=100)\n",
    "ddql = DeepDoubleQLearning(action_space=4, gamma=0.90, lr_v=0.01)\n",
    "eps = ConstantEpsilonDecay(0.3)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "ddql.learning(env = env, epsilon_schedule= eps, n_episodes=2000, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "mc.play(env=env, bracketer=bracketer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf55723681413fd",
   "metadata": {},
   "source": [
    "# Jupyter Snake\n",
    "\n",
    "---\n",
    "\n",
    "In this project our goal is to beat the game of snake, applying multiple RL techniques in order to teach an agent how to play the game.\\\n",
    "The project is divided in 3 parts:\n",
    "1. Developement of the Environment\n",
    "2. Implementation of the Algorithms\n",
    "3. Learning and evaluating phase of the Algorithms\n",
    "\n",
    "This whole project is developed as final project for the \"Reinforcement Learning\" course (2024-2025).\n",
    "\n",
    "Authors : *Bredariol Francesco, Savorgnan Enrico, Tic Ruben*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19772e2a20beeb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import *\n",
    "from eligibility_traces import *\n",
    "from epsilon_scheduler import * \n",
    "from snake_environment import *\n",
    "from states_bracket import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51add63e198d7ef2",
   "metadata": {},
   "source": [
    "## PART 1\n",
    "---\n",
    "*Environment*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5d07775a569af",
   "metadata": {},
   "source": [
    "### **The Game**\n",
    "\n",
    "For who who doesn't know Snake is a not just a game but a genre of action video games.\\\n",
    "It was born in 1976 competitive arcade video game Blockade, where the goal was to survive longer than others players while collecting the most possible food.\\\n",
    "In this game you control the head of a snake on grid world and you aim to eat some food in order to become bigger. The big difficulty here is that if you hit your tail (this is the only common rule for all snake variant) you die.\\\n",
    "There are multiple version of the game and some of them are really weird (where teleportation can occour or some food can actually make you die).\\\n",
    "We took in account the most basic version, where:\n",
    "\n",
    "1. The world is a discrete grid world of size $n\\times m$\n",
    "2. There is always only one food (apple) on the grid world, and when you it changes position\n",
    "3. There are no periodic boundary conditions, meaning that if you hit a wall you die\n",
    "\n",
    "The rest is just as described as in the introduction to the game.\\\n",
    "Little side note is that this version is inspired by the \"Snake Byte\" published by Sirius Software in 1982.\n",
    "\n",
    "### **The Implementation**\n",
    "\n",
    "Thanks to Gymnasium and PyGame the implementation of this simple version of the game is pretty straightforward.\\\n",
    "Developed completely in Python in the file \"snake_environment.py\", this implementation follows the Gym protocol, defining:\n",
    "\n",
    "1. Step\n",
    "2. Reset\n",
    "3. Render, which use PyGame\n",
    "4. Close\n",
    "\n",
    "All others functions are private and only used inside the class, for the exception of \"get_possible_action\" which is useful for our purpose.\n",
    "\n",
    "One important thing is that we actually defined a maximum number of step inside the environment to prevent infinte loop driven by bad policies while training. This is a parameter for the __init__ with default value 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817161e222fff38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "env = SnakeEnv(render_mode=\"human\")\n",
    "\n",
    "done, keep = False, True\n",
    "\n",
    "state, _ = env.reset()\n",
    "action = 0\n",
    "\n",
    "while not done and keep:\n",
    "    action = random.choice(env.get_possible_actions(action))\n",
    "    state, reward, done, trunc, inf = env.step(action)\n",
    "    keep = env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b624aa4d082126",
   "metadata": {},
   "source": [
    "### **The Dimensionality Problem**\n",
    "\n",
    "Once the environment is defined, one can think about how big the space of all possible configuration is.\\\n",
    "Well, doing some (pretty bad but although valid) approximation, considering a state as the matrix representation of the grid with 0 (empty cell), 1 (apple), 2(head) and 3 (tail), the dimension of all possible configuration ends up being something like this:\n",
    "$$\n",
    "    |S| = (n\\times m)(n\\times m)2^{(n\\times m)}\n",
    "$$\n",
    "This should describe all the possible positions of the apple, all the possible position of the head and all possible configuration on the grid of the tails (now this is the big approximation, since the tail configuration is not independent from the head position). Anyway, even if this is an approximation one can simply add the \"blocks\" on the grid world (static cells that kill, if touched, the snake) and the dimension should exactly being that big.\n",
    "\n",
    "Now this is not a simple thing to deal with while learning. Solution? Binning (or bracketing, how we actually call it). Now on this soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8940b47dbaa3e8b",
   "metadata": {},
   "source": [
    "## PART 2\n",
    "---\n",
    "*Algorithms*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec1f7cbeb8d4d9",
   "metadata": {},
   "source": [
    "## **BINNING (or Bracketing)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617b51c376f6feb",
   "metadata": {},
   "source": [
    "As shown before the Snake game has a huge state dimension.\\\n",
    "Since a look up table of that dimension has no logic to exist (I don't even think our computers can store something like that) and since it is pretty impossible just to see one example for each pair state-action in an entire life time, something had to come in mind.\\\n",
    "Thanks to a lesson we learnt about binning (that we called bracketing for the entire project, shame on Francesco) and decided to try it out.\\\n",
    "Binning essentially is just a supervised technique (it is the human that codifies how it works) that agglomerates similar states together, in order to reduce the dimensionality of the problem.\\\n",
    "A very stupid example could be the following: each state just randomly be labelled as 0 or 1. And the agents now will not see the entire state representation, but only the label you gave it. Now this example is stupid because, using this random strategy, you end up with no knowledge at all. BUT, look at what happened at your state dimension: it fell down from whatever it was to 2! Pretty neat, uh?\n",
    "\n",
    "We discussed together end decided to try a lot of different binning techniques, and we end up discovering the big tradeoff in this field: **information against dimension**.\\\n",
    "Minimal state binning are easy to implement, but if they are too small it is not ensured that they will bring enough information to actually learn to the agent. On the other hand, if you give too many information to the agent, you will end up again with a too big state dimension to deal with.\n",
    "\n",
    "Another important aspect of binning is that you actually \"completely\" lose the transition function below your system. It is true that you can develop a new transition function on a new space, the \"binning space\", but it is not easy and it is not ensured to be relevant.\n",
    "\n",
    "Now we will quickly see all the binnings we implemented. We defined in the states_bracket.py a super class (StateBracket) which implement the protocol for all the binning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ed10e74a7a5c8",
   "metadata": {},
   "source": [
    "### **FOOD INFORMATION ONLY**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bcd4ea63fa6cfc",
   "metadata": {},
   "source": [
    "Our first approach was to give only information about the position of the apple wrt the head of the snake. Two variants of this idea came up:\n",
    "\n",
    "1. Food Relative Position \n",
    "2. Food Direction\n",
    "\n",
    "Both of the techniques are pretty straightforward to implement, and their computation is very fast as well. However, the snake loses information about its tail, the walls and the boundaries of the cell.\n",
    "\n",
    "\n",
    "##### **Food Relative Position**\n",
    "\n",
    "Once you get the apple position $(a_y, a_x)$ and the head position $(h_y, h_x)$ you just return as state the tuple $(a_y - h_y, a_x - h_x)$. This approach reduces the states' dimension up to $2m \\times 2n$\n",
    "\n",
    "##### **Food Direction**\n",
    "\n",
    "Once you get the apple position $(a_y, a_x)$ and the head position $(h_y, h_x)$ you just return as state the tuple $(a_y < h_y, a_y > h_x, a_x < h_x, a_x > h_x)$.\\\n",
    "Now, while the first one is straightforward, this a little more subtle. This tells you whether your head is above, below, to the left, or to the right of the food. \\\n",
    "This is a very minimal information: the states are condensed into just $8$ bins.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a8b27963c1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.array([\n",
    "    [0, 0, 0], \n",
    "    [0, 0, 1], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "frp = FoodRelativePositionBracket()\n",
    "fd = FoodDirectionBracket()\n",
    "print(frp.bracket(state))\n",
    "print(fd.bracket(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e36cfa3f946f60",
   "metadata": {},
   "source": [
    "\n",
    "These binning techniques combines the things we have just seen plus information relative to the neighborhood of the head.\\\n",
    "Neighborhood could be both of Von Neumann ('V') type or Moore ('M') type.\n",
    "A Von Neumann neighborhood is defined as the four cells that are vertically or horizontally adjacent to the head of the snake.\n",
    "On the other hand, a Moore neighborhood considers vertically, horizontally and diagonally adjacent cells.\n",
    "\n",
    "The radius is a parameter for this binning. Notice that the greater the radius, the greater the total state dimension. The information within the neighborhood are expressed in the form of 0, 1, 2 and 3. 0 if a cell is free, 1 if a cell is occupied by the food, 2 if is occupied by the head and 3 if is occupied by the tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b9640abe822bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.array([\n",
    "    [0, 0, 0], \n",
    "    [0, 0, 1], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "npfrp = NeighPlusFoodRelativePositionBracket(neigh='V', radius=1)\n",
    "npfd = NeighPlusFoodDirectionBracket(radius=1)\n",
    "print(npfrp.bracket(state))\n",
    "print(npfd.bracket(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90f951d7a5f70c",
   "metadata": {},
   "source": [
    "### **ONLY NEIGHBORHOOD INFORMATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ec4a93bd73d92",
   "metadata": {},
   "source": [
    "An interesting experiment was the definition of the binning containing only information relative to the neighborhood of the head (adding the 2 value for the apple in this case).\\\n",
    "The idea was that, using this type of binning, the agent could have learnt to search being really careful, but probably with no knowledge on its own position it is impossible for it to learn a valid strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc272c9f6b931cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.array([\n",
    "    [0, 0, 0], \n",
    "    [0, 0, 1], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "n = NeighborhoodBracket()\n",
    "print(n.bracket(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32745fff2fe654ac",
   "metadata": {},
   "source": [
    "### **FOOD AND NEIGHBORHOOD AND TAIL INFORMATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ac59805ae8599",
   "metadata": {},
   "source": [
    "The last type of binning explored combines all the informations described until now plus a little information about the tail.\\\n",
    "The relative information about the tail is its length, and should help the agent learn to be a little more careful when the tail gets longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4566eae23b60b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.array([\n",
    "    [0, 0, 0], \n",
    "    [0, 0, 1], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "tpnpfrp = NeighPlusFoodRelativePositionPlusTailBracket()\n",
    "tpnpfd = NeighPlusFoodDirectionPlusTailBracket()\n",
    "print(tpnpfrp.bracket(state))\n",
    "print(tpnpfd.bracket(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71797dc42244ba39",
   "metadata": {},
   "source": [
    "## ALGORITHMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52247ba803bd388",
   "metadata": {},
   "source": [
    "### **Why not MDP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad610d32db6b503",
   "metadata": {},
   "source": [
    "\n",
    "How we have seen in the latest section, the only way to obtain a dealable state dimension is using the binning. \\\n",
    "The problem is that, using binning, we lose the transition function below our MDP. Indeed, being in the same state, taking the same action, ending into the same state, could possibly lead to different rewards.\n",
    "To give an example, consider the two situations below, where we marked with a $O$ the head of the snake, with $0$ its body and with $X$ the food (consider the snake without tail):\n",
    "\n",
    "\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " & 0 & 0 \\\\\n",
    "\\hline\n",
    " & O & 0 \\\\\n",
    "\\hline\n",
    "X &  &  \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " &  &  \\\\\n",
    "\\hline\n",
    " & O & 0 \\\\\n",
    "\\hline\n",
    "X &  & 0 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "Such situations are binned into the same condensed-state $(-1, -1)$. However, the action \"UP\" will lead in the first example to a negative reward because of the hit of the snake's tail, while  in the second case, to a zero reward.\n",
    "\n",
    "This erases the capability to retrieve good results solving the problem as an MDP via policy iteration.\n",
    "\n",
    "TO ARGUMENT A BIT MORE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c81a7f91a0d5aa",
   "metadata": {},
   "source": [
    "### **Our Choices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a738ef59f9a9a",
   "metadata": {},
   "source": [
    "\n",
    "In the end we decided to develop 5 different **Model-free algorithms** (with also some variants of them) in order to take familiarity with the whole RL framework. These are the algorithms we implemented:\n",
    "\n",
    "1. Montecarlo \n",
    "2. SARSA\n",
    "3. QLearning\n",
    "4. DDQL\n",
    "5. Policy Gradient\n",
    "\n",
    "We firstly implemented a super class that defines a protocol for all the algorithms and provides useful function such as \"get_action_epsilon_greedy\" and so on.\\\n",
    "Useful method of the class are the save and upload methods, which can let to store the results obtained as a pickle dictionary in order to retrieve it later.\\\n",
    "In addiction the utils.py contains a lot of useful function used to deal with the default_dict, a structure we used to store the QValues look up table (for the algorithms that require it). Since we used a lot of binning we used default dict to possibly deal with no-fixed state dimension.\n",
    "\n",
    "Before diving in our actual implementation, let's briefly remind some key concepts that will be used.\n",
    "\n",
    "*Credit for the key concepts to [this tutorial](https://medium.com/@hsinhungw/intro-to-reinforcement-learning-monte-carlo-to-policy-gradient-1c7ede4eed6e)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a05f54305a18c5c",
   "metadata": {},
   "source": [
    "### **Policy-based vs Value-based**\n",
    "\n",
    "Most of our algorithms are value-based but policy gradient is in fact policy-based. Let's define the difference between these two classes.\n",
    "\n",
    "1. **Policy-based methods**: The agent learns the policy directly.\n",
    "2. **Value-based methods**: The agent learns a value function that gives the expected return of being in a given state or performing a given action in a given state. The policy can then be derived from the learned value function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3705c422f85849",
   "metadata": {},
   "source": [
    "### **Off-policy / On-policy**\n",
    "\n",
    "In RL, the agent generates experience by interacting with the environment under a certain policy and then learns the policy from those experiences. Given this in mind, algorithms can use different approaches to the policy. Two main classes exist, let's break them down.\n",
    "\n",
    "\n",
    "1. **On-policy methods**: The agent attempts to learn the policy that is also used to generate the experience.\n",
    "2. **Off-policy methods**: The agent learns a policy that is different from the one used to generate experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8e80422b412f3",
   "metadata": {},
   "source": [
    "### **Q-Value Learning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d0d8b11d038288",
   "metadata": {},
   "source": [
    "Our goal is to find the optimal policy given only the experience of an environment. Again, the experience consists in the trajectories we perform as we explore the environment\n",
    "\n",
    "$\\tau_i = (S^i_0, A^i_0, R^i_1, S^i_1, A^i_1, R^i_1, \\dots S^i_{T^i})$\n",
    "\n",
    "\n",
    "To do this, it will be more convenient to consider, instead of the _state_-function $V_{\\pi} (s) $, the _state/action_-function $Q_{\\pi}(s,a)$. It is defined as:\n",
    "\n",
    "$$\n",
    "Q_{\\pi}(s,a) = \\mathbb{E}_\\pi\\bigg[ \\sum_{t=0}^\\infty \\gamma^t \\, R_{t+1} \\, \\, \\Big| \\, \\, S_0 = s, A_0 = a \\bigg] \\ .\n",
    "$$\n",
    "\n",
    "**Definition 1** _\"The expectation discounted cumulative sum of all future rewards when starting from state $s$, acting with action $a$ and then always following the policy $\\pi$\"_\n",
    "\n",
    "or equivalently as:\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) = \\mathbb{E}_{\\pi}\\bigg[ R_1 + \\gamma V_\\pi(S_1) \\,\\, \\Big| \\,\\, S_0 = s, A_0 = a\\bigg] \\ .\n",
    "$$\n",
    "\n",
    "**Definition 2** _\"The expectation value of the immediate reward plus the discounted $V$-value of the following state $S_1$, when starting from state $s$, acting with action $a$ and then always following the policy $\\pi$.\"_\n",
    "\n",
    "or again as:\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) = \\mathbb{E}_{\\pi}\\bigg[ R_1 + \\gamma  \\, \\sum_{a'} \\, Q_\\pi(S_1, a') \\, \\pi(a' | S_1) \\,\\, \\Big| \\,\\, S_0 = s, A_0 = a\\bigg] \\ .\n",
    "$$\n",
    "**Definition 3** _\"The expectation value of the immediate reward plus the discounted $Q$-value of the following state $S_1$ and all possible actions $a'$ weighted by the probability of taking that action ($\\pi(a' | S_1)$), when starting from state $s$, acting with action $a$ and then always following the policy $\\pi$.\"_\n",
    "\n",
    "*Credit to the 4th tutorial for the definition*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf07ce9ff3616f2",
   "metadata": {},
   "source": [
    "### **Montecarlo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea81ab41f47ad5a4",
   "metadata": {},
   "source": [
    "\n",
    "Monte Carlo methods are ways of estimating value functions through experiences (sample sequence of states, actions, and rewards). Recall that the value of a state is the expected return (expected cumulative future discounted reward) starting from that state. One way to estimate the expected return is simply to average the returns observed after visits to that state. As more returns are observed, by the law of large numbers, the average should converge to the expected return. This idea underlies all Monte Carlo methods.\n",
    "\n",
    "Our first algorithm developed was the **on-policy Montecarlo**. It works as follows:\n",
    "\n",
    "1. On-policy MC uses an **ε-soft policy**. A soft policy refers to any policy that has a small, but finite, probability of selecting any possible action, ensuring exploration of alternative actions.\n",
    "2. After each episode, the **observed returns** are used to learn the q-value function, and then the policy is improved based on the learned value function for all the states visited in the episode.\n",
    "\n",
    "This means that we use the first definition. This definition has no bias but an high variance.\n",
    "\n",
    "![onpolicymc](./images/on_policy_mc_correct.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6731a5a12d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=100)\n",
    "mc = Montecarlo(action_space=4, gamma=0.90, lr_v=0.01)\n",
    "eps = ConstantEpsilonDecay(0.3)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "mc.learning(env = env, epsilon_schedule= eps, n_episodes=400, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "mc.play(env=env, bracketer=bracketer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62668643763437e7",
   "metadata": {},
   "source": [
    "### **SARSA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693be9de27e12142",
   "metadata": {},
   "source": [
    "SARSA is a fascinating on-policy method which use state action and reward (State Action Reward State Action) in order to achieve its goals. There are many variants of SARSA, and all of them are different in terms of how they update the Q-Value table. ```(ENRICO) Quali versioni? -> Expected Sarsa, Sarsa, lambda Sarsa? Forse sono solo expected sarsa e sarsa```\n",
    "\n",
    "We implemented both a SARSA(0) version, where the Q-values are updated each step, and a SARSA(λ). \\\n",
    "The second option, despite being a bit slower, is a more general and flexible algorithm which leaves the possibility to see more than one action at the time, so to have a more accurate update of the Q-values. In this version of SARSA we implemented a \"backward approach\" thanks to eligibility traces.\n",
    "\n",
    "The (common) pseudocode for SARSA(0) is here below:\n",
    "\n",
    "![sarsa](./images/sarsa.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915e77f85cbafa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=100)\n",
    "sarsa = SARSA(action_space=4, gamma=0.90, lr_v=0.01)\n",
    "eps = ConstantEpsilonDecay(0.3)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "sarsa.learning(env = env, epsilon_schedule= eps, n_episodes=500, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "sarsa.play(env=env, bracketer=bracketer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47190adcedcc75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=100)\n",
    "sarsal = SARSALambda(action_space=4, gamma=0.90, lr_v=0.01, lambda_value=0.1)\n",
    "eps = ConstantEpsilonDecay(0.3)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "sarsal.learning(env = env, epsilon_schedule= eps, n_episodes=500, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "sarsal.play(env=env, bracketer=bracketer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4191100147e1d909",
   "metadata": {},
   "source": [
    "### **QLearning**\n",
    "\n",
    "QLearning is the first off-policy algorithm we see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c6b34262d2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=1000)\n",
    "ql = QLearning(action_space=4, gamma=0.90, lr_v=0.01)\n",
    "eps = LinearEpsilonDecay(1, 0.999, 0.2)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "ql.learning(env = env, epsilon_schedule= eps, n_episodes=20000, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "ql.play(env=env, bracketer=bracketer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74fe1a704954b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ql.play(env=env, bracketer=bracketer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298bed3fe2e31f4",
   "metadata": {},
   "source": [
    "### **Double Deep Q Learning (DDQL)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76efa08f9109136b",
   "metadata": {},
   "source": [
    "This algorithm is a variant of the Q-Learning algorithm, which is an off-policy method.\n",
    "\n",
    "It is basically a combination of Q-Learning and Deep Q-Networks (DQN), used to estimate the Q-values.\n",
    "The main idea behind DDQL is to use two separate neural networks for the estimation, which helps to reduce the overestimation bias that can occur in Deep Q-learning and in Q-learning in general.\n",
    "Indeed, in common Q-Learning, the use of the $\\max$ operator may lead to inflated Q-values. This consequently leads to suboptimal policies, especially in complex environments.\n",
    "\n",
    "DDQL addresses the problem by exploiting a **Online neural network** for choosing the best action given a certain state, and using a **Target neural network** for computing the estimated Q-values given the couple (state, action).\n",
    "The target network is updated less frequently than the online one, so the two are basically decoupled, and their predictions are independent. This helps to stabilize the learning process and reduce the overestimation bias.\n",
    "\n",
    "\\begin{ENRICO}\n",
    "Ricordati di parlare di replay buffer\n",
    "\\end{ENRICO}\n",
    "\n",
    "![ddql](./images/ddql.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa770a00baedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=100)\n",
    "ddql = DeepDoubleQLearning(\n",
    "    action_space=4,\n",
    "    state_dim=bracketer.get_state_dim(),\n",
    "    gamma=0.90,\n",
    "    lr_v=0.001,\n",
    "    batch_size=128,\n",
    "    memory_size=10000,\n",
    "    target_update_freq=200,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "eps = ConstantEpsilonDecay(0.3)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "ddql.learning(env = env, epsilon_schedule= eps, n_episodes=2000, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "ddql.play(env=env, bracketer=bracketer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb08d1500ccc6ad",
   "metadata": {},
   "source": [
    "#### Main results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c6152529683c5",
   "metadata": {},
   "source": [
    "Several DDQL architectures have been trained.\n",
    "\n",
    "For all of them, the following hyperparameters are kept fixed:\n",
    "- *ε*: we chose a linearly-decaying epsilon for the ε-greedy policy, starting from ε=1 to ε=0.05, with a slope of -0.999.\n",
    "- *learning rate*: constant at 0.001\n",
    "- *memory size*: the maximum size of the memory buffer, fixed to 10000.\n",
    "- *batch size*: the number of samples from the memory buffer, fixed to 128.\n",
    "- *frequency of update of the target neural network*: 200 step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955e204",
   "metadata": {},
   "source": [
    "## **Policy Gradient**\n",
    "\n",
    "Up untill now, we focused on determining the state-action value function Q. The policy was only a subproduct of Q. \\\n",
    "We might wonder whether it is possible to approximate the optimal policy **directly**.\n",
    "The answer is Policy Gradient.\n",
    "\n",
    "### **Theory**\n",
    "\n",
    "#### Policy Gradient Theorem\n",
    "\n",
    "Let G be the average return. Let $ \\pi{_\\theta}$ be a parametrized policy. Then we have that: \n",
    "$$\n",
    "\\nabla_\\theta G = \\sum_{s a} \\pi(a|s)\\,\\eta_\\pi(s)\\,Q(s, a)\\; \\nabla_\\theta \\log(\\pi(a|s)) \n",
    "$$\n",
    "The powerful aspect of this equation is this: the summatory is obtained expermentally. The algorithm needs only to consider $Q(s, a)\\; \\nabla_\\theta \\log(\\pi(a|s))$ for every step. In fact we might rewrite the same equation as follows: \n",
    "$$\n",
    "\\nabla_\\theta G = E_\\pi \\left[ \\sum_{t = 0}^{\\infty} Q(S_t, A_t)\\; \\nabla_\\theta \\log(\\pi(A_t|S_t))\\qquad | \\qquad S_0 \\sim \\rho_0 \\; A \\sim \\pi \\right]\\qquad (*)\n",
    "$$\n",
    "\n",
    "From now on we will use the **Softmax parametrization** in a tabular contex. \n",
    "\n",
    "\n",
    "#### Actor Only\n",
    "\n",
    "We can use the equation (*) directly. Since the policy must remain fixed as we calculate the gradient, we update at the end of each episode. Thus, we can use a Monte Carlo estimator for $Q$. Here we show a simple pseudocode:\n",
    "\n",
    "collect  $\\{S_t, A_t, R_t\\}_{t= 0, 1, ..., T}$ \n",
    "\n",
    "$G = 0$ \\\n",
    "for $t = T, T-1, ... 0$ \\\n",
    "$\\qquad G = R_t + \\gamma G$ \\\n",
    "$\\qquad \\theta = \\theta +  \\alpha G \\nabla_\\theta \\log(\\pi(A_t | S_t))$\n",
    "\n",
    "Here $\\alpha$ is a fixed learning rate. In the case of softmax, we have that $\\theta = \\{ \\theta_{sa}\\} \\text{ with } s \\in S \\; a \\in A$. Then, the gradient is \n",
    "$$\n",
    "\\nabla_\\theta \\log(\\pi(A_t | S_t))_{\\theta_{s,a}} = \n",
    "\\begin{cases}\n",
    "1 - \\pi(A_t | S_t)\\qquad &s, a = S_t, A_t \\\\\n",
    "-\\pi(A_t | S_t) \\qquad &s = S_t \\; a \\neq A_t \\\\\n",
    "0 \\qquad &\\text{else}\n",
    "\\end {cases}\n",
    "$$\n",
    "\n",
    "There is an intuitive way to interpret this code: if, in a certain state $s$, an action $a$ yielded good results, we increase $\\theta_{s, a}$ and decrease $\\theta$ of the other actions in the same state.\n",
    "\n",
    "\n",
    "#### Actor Critic\n",
    "\n",
    "Actor Only is a very simple, but not efficient approach. The main problem is that we value an action through the return obtained afterwards, but *without a contex*. A same return can be very suboptimal, if the sistem is generous, or very good, if rewards are rare. A better and equivalent way to estimate $\\nabla_\\theta G$ is through the *advantage*, defined as:\n",
    "$$\n",
    "Ad(s, a) = Q(s, a) - V(s)\n",
    "$$\n",
    "\n",
    "If we use bootstrapping we can write \n",
    "$$\n",
    "Ad(s, a) = R_t + \\gamma V(S_{t+1}) - V(S_t) = \\delta_t\n",
    "$$\n",
    "\n",
    "And we already have a way to estimate $V(s)$: Temporal Different Learning. We can combine the two, obtaining what is called Actor Critic. In fact we have a Critic, which estimates $V$, and an Actor, which estimates $\\pi$ through gradient ascent. We present here a simple pseudocode:\n",
    "\n",
    "collect $S_t, A_t, R_t, S_{t+1}$ \n",
    "\n",
    "$\\delta = R_t + \\gamma V(S_{t+1}) - V(S_t)$ \\\n",
    "$V(S_t) = V(S_t) + \\alpha_V \\delta $\n",
    "\n",
    "$\\theta = \\theta +  \\alpha_A \\delta \\nabla_\\theta \\log(\\pi(A_t | S_t))$\n",
    "\n",
    "For the learning rates, we ask that $\\alpha_V > \\alpha_A$. In fact the Actor is using the Critic for its estimation, thus the Critic must learn faster than the Actor.\n",
    "\n",
    "\n",
    "#### General Advantage Estimation\n",
    "\n",
    "With Actor Critic algorithm we have to update the policy at every step. This can be time consuming. Furthermore, it could be better to update $\\pi$ only at episode ended, so that we can have a better idea on the long term performance of the policy. For that we use an algorithm called General Advantage Estimation (GAE).\n",
    "\n",
    "So far we estimated $Ad$ with only the current reward. But, since we update at the end of the episode, we can actually consider $n$ future rewards, obtaining:\n",
    "$$\n",
    "\n",
    "Ad_t^n = R_t + \\gamma R_{t+1} + ... + \\gamma^n R_{t + n} + \\gamma^{n+1} V(S_{t+n+1}) \\;- \\; V(S_t) = [R_t -  V(S_t)] \\; +\\; \\gamma R_{t+1} + ... + \\gamma^n R_{t + n} + \\gamma^{n+1} V(S_{t+n+1}) \\\\\n",
    "       = \\delta_t + \\gamma[ R_{t+1} - V(S_{t+1})] + ... + \\gamma^n R_{t + n} + \\gamma^{n+1} V(S_{t+n+1}) = ... = \\delta_t + \\delta_{t+1} + ... + \\delta_{t+n}\n",
    "\n",
    "$$\n",
    "The idea behind GAE is to consider $Ad^n$ for all $n$, each discounted by $\\lambda^n$. Specifically \n",
    "$$\n",
    "Ad_t^{GAE} = (1-\\lambda) \\sum_{n = 0}^{\\infty} \\lambda^nAd_t^n = (1-\\lambda) \\left[ \\delta_t(1 + \\lambda + \\lambda^2 + ...) + \\gamma \\delta_{t+1}(\\lambda + \\lambda^2 + ...) + ...\\right] = \\sum_{n = 0}^{\\infty} (\\lambda\\gamma)^n\\delta_{t+n} \n",
    "$$\n",
    "\n",
    "The advantage can be seen also in another way. If we conider the average of $Ad$ with fixed state-action, we obtain the advantage. Instaed, if we consider the average of $Ad$ with a fixed state, we obtain a measure of how much owr estimator is correct. Indeed we will use $Ad$ both for estimating $V$ and $\\pi$. Here we present a pseudocode: \n",
    "\n",
    "collect  $\\{S_t, A_t, R_t\\}_{t= 0, 1, ..., T}$ \n",
    "\n",
    "$Ad = 0$ \\\n",
    "for $t = T, T-1, ... 0$ \\\n",
    "$\\qquad \\delta = R_t + \\gamma V(S_{t+1}) - V(S_t) $\\\n",
    "$\\qquad Ad = \\delta_t + \\lambda\\gamma Ad$ \n",
    "\n",
    "$\\qquad V(S_t) = V(S_t) + \\alpha_V\\, Ad$\n",
    "\n",
    "$\\qquad \\theta = \\theta +  \\alpha_A\\; Ad \\;\\nabla_\\theta \\log(\\pi(A_t | S_t))$\n",
    "\n",
    "With this algorithm we have an interesting variabile in hand: $\\lambda$. If $\\lambda$ is close to 0, we learn only from close rewards. Thus we will have less variance, but there will be a bias due to bootstrapping. If, on the contrary, $\\lambda$ is close to 1, distance rewards have a great influence. Thus variance will increase, but there will be less bias. In fact we are moving toward a Monte Carlo apprach, notoriously unbiased.\n",
    "\n",
    "A little note: the algorithm presented here is actually a variation of the standard GAE. In fact we are calculating the advantages, and at the same time updating the values. Usually the advantages are calculated first, and then the values are updated. This is clearly a convenient approach for batch training with Neural Networks. Here we are not using NNs, and then we get to choose one of the two approaches. The \"GAE in place\" algorithm has been chosen because it exhibits lower variance. Infact $\\delta_t$ is calculated with a value function already updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828d4040",
   "metadata": {},
   "source": [
    "### **Implementation**\n",
    "\n",
    "As already discussed, the dimentionality of the problem forces us to use binnings. For the rest of the text, we will improperly call state what is actually a bin.\n",
    "\n",
    "All the algorithms developed in this project are subclasses of one general class. In it we define a method called \"learning\", which is the backbone of all the algorithms. Since its implementation presents some technical details, we show simply a pseudocode:\n",
    "\n",
    "for each episode:\n",
    "\n",
    "$\\qquad$ initialize $S_0$\\\n",
    "$\\qquad$ take $A_0 \\sim \\pi(\\,\\cdot\\,|\\,S_0\\,)$ --> here we call \"get_action_during_learning(state)\"\\\n",
    "$\\qquad$ loop over t untill termination:\n",
    "\n",
    "$\\qquad \\qquad$ observe $S_{t + 1} \\; R_{t + 1}$\\\n",
    "$\\qquad \\qquad$ append $(S_t, A_t, R_{t+1})$ to episode\\\n",
    "$\\qquad \\qquad$ take $A_{t+1} \\sim \\pi(\\,\\cdot\\,|\\,S_{t+1}\\,)$ --> here we call \"get_action_during_learning(state)\"\\\n",
    "$\\qquad \\qquad$ if needed, do step update --> here we call \"single_step_update($S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, \\text{done}$)\"\n",
    "\n",
    "$\\qquad$ if needed, do episode update --> here we call \"single_episode_update(episode)\"\n",
    "    \n",
    "Let us dive deeper into the implementation. Here we have some common elements used in the algorithms:\\\n",
    "    1) \"parameters\" is a numpy array containing $\\theta_{sa}$ for all state-action pairs.\\\n",
    "    2) If used, \"value\" is a numpy array containing $V(s)$ for all states.\\\n",
    "    3) \"index_sa\" is a utility that associates a state-action pair to an index for the arrays.\\\n",
    "    4) \"index_s\" is a utility that associates a state to an index for the arrays.\\\n",
    "    5) \"policy\" is a utility that takes as input an action and a state, performes the softmax, and returns the probability of taking that action, given that we are in the inserted state.\n",
    "\n",
    "Of these five elements, only the last one is theoretically interesting, thus we show its implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ec965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(self, action, state):\n",
    "        # This functions performes the Softmax, obtaining prob(A = action | S = state ) = exp( parameters(s, a) )  /  (normaliz. factor over actions)\n",
    "\n",
    "        return np.exp(self.parameters[self.index_sa((*state, action))])  /  sum(  np.exp(self.parameters[self.index_sa((*state, a))])  for a in range(self.action_space)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c87b32",
   "metadata": {},
   "source": [
    "With this last utility we can implement the method \"get_action_during_learning\" as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f2944822ebb3c",
   "metadata": {},
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_during_learning(self, state, possible_actions=None):     \n",
    "    \n",
    "    prob_a = np.zeros(self.action_space)\n",
    "\n",
    "    for a in range(self.action_space):\n",
    "        prob_a[a] = self.policy(a, state)       #we construct the probability distribution of actions following the policy in the current state \n",
    "    \n",
    "    chosen_action = np.random.choice(range(self.action_space), p=prob_a)        #we choose an action following the distribution \"prob_a\"\n",
    "    return chosen_action\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884eb1a9",
   "metadata": {},
   "source": [
    "\n",
    "We still need to see the realization of \"single_step_update\" and \"single_episode_update\". For that we need to analyze each algorithm separately.\n",
    "\n",
    "#### Actor Only\n",
    "\n",
    "This strategy has to be considered just as an intermediate step. In fact it is too simplistic and doesn't have clear advantages. Nonetheless we show it for completeness. As seen before, we need only to update the policy at the end of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_episode_update(self, episode):\n",
    "    # After the episode ended, we update the policy\n",
    "    # The policy is parametrized via soft-max, and theta is a vector with entries for every pair state-action\n",
    "    G = 0\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = reward + self.gamma * G\n",
    "\n",
    "        # Update the parameters\n",
    "        #                                       1 - pi(a|s)   if a is the performed action\n",
    "        # we recall that grad log(pi(a|s)) = \n",
    "        #                                       - pi(a|s)     else\n",
    "\n",
    "        for a in range(self.action_space):\n",
    "            if a == action:  #for the performed action\n",
    "                self.parameters[self.index_sa((*state, a))] += self.lr_a*G*(1-self.policy(action, state))\n",
    "            else:\n",
    "                self.parameters[self.index_sa((*state, a))] += self.lr_a*G*( -self.policy(action, state))\n",
    "\n",
    "        #for numerical stability, we subtract from the parameters their max value\n",
    "        max_parameter = np.max( [self.parameters[self.index_sa((*state, a))]   for a in range(self.action_space)])\n",
    "        for a in range(self.action_space):\n",
    "            self.parameters[self.index_sa((*state, a))] -= float(max_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62541576",
   "metadata": {},
   "source": [
    "At the end of the function we see a technical detail. If we analyze the function \"policy\", we notice that we calculate the exponential of the parameters. If these numbers are too large, we encounter numerical problems. On the other hand, we don't find this issue for large negative numbers. For dealing with such a problem, we can use the following propiety of the Softmax:\n",
    "$$\n",
    "\\frac{e^{\\theta}}{\\sum_{\\theta'} e^{\\theta'}} = \\frac{e^{\\theta -M}}{\\sum_{\\theta'} e^{\\theta'-M}} \\qquad \\forall M \\in \\mathbb{R}\n",
    "$$\n",
    "Since we want to minimize the parameters, we subtract their maximum every time we modify them.\n",
    "\n",
    "#### Actor Critic\n",
    "\n",
    "For this algorithm we update at every step, thus we override only \"single_step_update\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446dce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_step_update(self, s, a, r, new_s, new_a, done):\n",
    "    # After the episode ended, we update the value and the policy\n",
    "    # The policy is parametrized via soft-max, and theta is a vector with entries for all pair state-action\n",
    "\n",
    "    # critic update\n",
    "    if done:   \n",
    "        delta = r + 0 - self.value[self.index_s(s)]\n",
    "    else:\n",
    "        delta = r + self.gamma*self.value[self.index_s(new_s)]    -    self.value[self.index_s(s)]\n",
    "\n",
    "    self.value[self.index_s(s)] += self.lr_v*delta\n",
    "\n",
    "    # actor update (parameters)\n",
    "    #                                       1 - pi(a|s)   if a is the performed action\n",
    "    # we recall that grad log(pi(a|s)) = \n",
    "    #                                       - pi(a|s)     else\n",
    "    for ap in range(self.action_space):\n",
    "        if ap == a:  #for the performed action\n",
    "            self.parameters[self.index_sa((*s, ap))] += self.lr_a*delta*(1-self.policy(a, s))\n",
    "        else:\n",
    "            self.parameters[self.index_sa((*s, ap))] += self.lr_a*delta*( -self.policy(a, s))\n",
    "\n",
    "    # For numerical stability, we subtract from the parameters their max value\n",
    "    max_parameter = np.max( [self.parameters[self.index_sa((*s, ap))]   for ap in range(self.action_space)])\n",
    "    for ap in range(self.action_space):\n",
    "        self.parameters[self.index_sa((*s, ap))] -= float(max_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a75cae",
   "metadata": {},
   "source": [
    "We note that we have:\n",
    "$$\n",
    "\\alpha_A = \\text{lr\\_a} \\qquad \\alpha_V = \\text{lr\\_v}\n",
    "$$\n",
    "#### GAE\n",
    "\n",
    "For GAE we update the policy and value function only at the end of each episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_episode_update(self, episode):\n",
    "    # After the episode ended, we update the value and the policy\n",
    "    # The policy is parametrized via soft-max, and theta is a vector with entries for every couple bin-action\n",
    "\n",
    "    GAE = 0       # GAE is a variable used to store the Generalized Advantage Estimator for all t \n",
    "\n",
    "    for t in range(len(episode) -1, -1, -1):\n",
    "        \n",
    "        state, action, reward = episode[t]\n",
    "\n",
    "        if t != len(episode) -1:\n",
    "            next_state = episode[t+1][0]\n",
    "\n",
    "            delta = reward + self.gamma*self.value[self.index_s(next_state)] - self.value[self.index_s(state)]\n",
    "        else:\n",
    "\n",
    "            delta = reward - self.value[self.index_s(state)]\n",
    "\n",
    "        GAE = delta + self.gamma*self.Lambda*GAE        #GAE is overwritten at each step, and we have GAE = GAE(t)\n",
    "        \n",
    "        #update the value\n",
    "\n",
    "        self.value[self.index_s(state)] += self.lr_v*GAE\n",
    "\n",
    "        # Update the parameters\n",
    "        #                                       1 - pi(a|s)   if a is the performed action\n",
    "        # we recall that grad log(pi(a|s)) = \n",
    "        #                                       - pi(a|s)     else\n",
    "\n",
    "        for a in range(self.action_space):\n",
    "            if a == action:  #for the performed action\n",
    "                self.parameters[self.index_sa((*state, a))] += self.lr_a*GAE*(1-self.policy(action, state))\n",
    "            else:\n",
    "                self.parameters[self.index_sa((*state, a))] += self.lr_a*GAE*( -self.policy(action, state))\n",
    "            \n",
    "\n",
    "        #for numerical stability, we subtract from the parameters their max value\n",
    "        \n",
    "        max_parameter = np.max( [self.parameters[self.index_sa((*state, a))]   for a in range(self.action_space)])\n",
    "        for a in range(self.action_space):\n",
    "            self.parameters[self.index_sa((*state, a))] -= float(max_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580d7da",
   "metadata": {},
   "source": [
    "We note that, also here, we have:\n",
    "$$\n",
    "\\alpha_A = \\text{lr\\_a} \\qquad \\alpha_V = \\text{lr\\_v}\n",
    "$$\n",
    "### **Results**\n",
    "\n",
    "For all the results shown in this chapter, we used as bin the 8 directions of food (sud, sud-est, est, ...) and a neighborhood of the snake's head. The total number of bins can be calculated in this way: \n",
    "$$\n",
    "N = n_{\\text{food directions}} \\cdot 2^{\\text{size neighborhood}} = 8 \\cdot 2^{\\text{size neighborhood}}\\qquad (*)\n",
    "$$\n",
    "\n",
    "As a target return, we have built a simple reference policy. It sais: don't go against a block, if possible go towards food, else go randomly in a free direction. For all examples, unless stated otherwise, we set reward food = 20, penality step = 0.2, penality death = 10. With this data, the average performance of this policy is around 400. \n",
    "\n",
    "#### Using Von Neuemann neighborhood radius 1\n",
    "\n",
    "For this part, we have learned policies using as bin the Von Neuemann radius 1 neighborhood plus the direction of food. This binning has been the most successfull one, since the number of different bins is small enough for a thorough learning. Specifically, using (*) the total number of bins is 128. \n",
    "\n",
    "Let us start with the most successfull learining: Actor Critic with lr_v = 0.2, lr_a = 0.01. With such a little knowledge of the state, the reference policy we built might seem as the optimal one. But here the surprise:\n",
    "\n",
    "<img src=\"images/V1_0.2_0.01_-_(20,-0.2,-10).png\" width=600>\n",
    "\n",
    "As we can see, owr algorithm finds even a better policy. In a nushell, it learns how to coil up, in order to reduce the risk of getting blocked within its own tail. \n",
    "\n",
    "This result was possible thanks to a careful tuning of the paramters. For example, if the learning rates are too high, the policy convinces itself of wrong strategies:\n",
    "\n",
    "<img src=\"images/lr_troppo_alto_AC_0.9_0.09_-_(20,-0.2,-10).png\" width=400>\n",
    "\n",
    "<img src=\"images/V1_0.9_0.09_-_(20,-0.2,-10).png\" width=400>\n",
    "\n",
    "\n",
    "Here we can see that:\\\n",
    "    -the food is est\\\n",
    "    -there are blocks to the north and west\\\n",
    "    -the policy is collapsed and gives with probability 1 the action: go north\n",
    "\n",
    "This example was performed with Actor Critic, lr_v = 0.9, lr_a = 0.09. The learining rates are indeed too large, and the learning is killed.\n",
    "\n",
    "Also GAE performes well in this contex, for any value of $\\lambda$:\n",
    "\n",
    "<img src=\"images/V1_0.1_0.005_0.2_(20,-0.2,-10).png\" width=400>\n",
    "\n",
    "<img src=\"images/V1_0.05_0.003_0.9_(20,-0.2,-10).png\" width=400>\n",
    "\n",
    "We can see that for this binning bootstrapping is a good idea. In fact, since the states aren't much, the knowledge of them is valuable.\n",
    "\n",
    "The last two plots give us the opportunity to point out that the laerning rates must be smaller for a larger $\\lambda$. This comes from the fact that $\\lambda$ increases variance, as discussed before. If, instead, we keep the same learning rates, the training is too unstable:\n",
    "\n",
    "<img src=\"images/V1_0.1_0.005_0.9_(20,-0.2,-10).png\" width=400>\n",
    "\n",
    "#### Using Moore neighborhood radius 1\n",
    "\n",
    "The net step in complexity is the Moore neighborhood. Using (*) one again, we obtain that the total number of bins, or states in owr improper notation, is 2048. It turns out that the tabular algorithms developed in this section have already an hard time learining. We obtain a \"not so good\" result with GAE lr_v = 0.09 lr_a = 0.004 and Lambda = 0.2. We take the opportunity to show also the effect of something called \"reward shaping\". If we set reward food = 10, penality step = 0.5, penality death = -10, the learning is killed. On the left we see the original learning, and on the right the training with the same paramteres, but with the improper rewards:\n",
    "\n",
    "<img src=\"images/M1_0.09_0.004_0.2_(20,-0.2,-10).png\" width=400>\n",
    "\n",
    "<img src=\"images/M1_0.09_0.004_0.2_(10,-0.5,-10).png\" width=400>\n",
    "\n",
    "#### Using Von Neuemann radius 2\n",
    "\n",
    "Here we move even further down complexity of state representation. The number of total bins is 32 000. Clearely ufeasible for owr tabular algorithms. Nonetheless we get to analyze better the difference between Actor Critic and GAE($\\lambda$). \n",
    "\n",
    "With Actor Critic we use bootstrapping a lot, whereas with GAE($\\lambda$) we use also the knowledge of future rewards, which are valueble fresh data from the enviroment. When the number of states was small, Actor Critic worked successfully because the value function is estimated well. When the states are a lot, we exept to have a less valueble estimation of the value function, making Actor Critic algorithms less efficient. In fact with Actor Critic, lr_v = 0.5, lr_a = 0.02 we have:\n",
    "\n",
    "<img src=\"images/V2_0.5_0.02_-_(20,-0.2,-10).png\" width=400>\n",
    "\n",
    "Instead, we obtain a good result with GAE lr_v = 0.1 lr_a = 0.005 Lambda = 0.5 : \n",
    "\n",
    "<img src=\"images/V2_0.1_0.005_0.5_(20,-0.2,-10).png\" width=400>\n",
    "\n",
    "We obtain surely a better result, but we are still far from the policy obtained with Von Neuemann radius 1, which scored up to 500. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

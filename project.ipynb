{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Jupyter Snake\n",
    "\n",
    "---\n",
    "\n",
    "In this project our goal is to beat the game of snake, applying multiple RL techniques in order to teach an agent how to play the game.\\\n",
    "The project is divided in 3 parts:\n",
    "1. Developement of the Environment\n",
    "2. Implementation of the Algorithms\n",
    "3. Learning and evaluating phase of the Algorithms\n",
    "\n",
    "This whole project is developed as final project for the \"Reinforcement Learning\" course (2024-2025).\n",
    "\n",
    "Authors : *Bredariol Francesco, Savorgnan Enrico, Tic Ruben*"
   ],
   "id": "ddf55723681413fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from algorithms import *\n",
    "from eligibility_traces import *\n",
    "from epsilon_scheduler import * \n",
    "from snake_environment import *\n",
    "from states_bracket import *\n",
    "from utils import *"
   ],
   "id": "19772e2a20beeb21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PART 1\n",
    "---\n",
    "*Environment*"
   ],
   "id": "51add63e198d7ef2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **The Game**\n",
    "\n",
    "For who who doesn't know Snake is a not just a game but a genre of action video games.\\\n",
    "It was born in 1976 competitive arcade video game Blockade, where the goal was to survive longer than others players while collecting the most possible food.\\\n",
    "In this game you control the head of a snake on grid world and you aim to eat some food in order to become bigger. The big difficulty here is that if you hit your tail (this is the only common rule for all snake variant) you die.\\\n",
    "There are multiple version of the game and some of them are really weird (where teleportation can occour or some food can actually make you die).\\\n",
    "We took in account the most basic version, where:\n",
    "\n",
    "1. The world is a discrete grid world of size $n\\times m$\n",
    "2. There is always only one food (apple) on the grid world, and when you it changes position\n",
    "3. There are no periodic boundary conditions, meaning that if you hit a wall you die\n",
    "\n",
    "The rest is just as described as in the introduction to the game.\\\n",
    "Little side note is that this version is inspired by the \"Snake Byte\" published by Sirius Software in 1982.\n",
    "\n",
    "### **The Implementation**\n",
    "\n",
    "Thanks to Gymnasium and PyGame the implementation of this simple version of the game is pretty straightforward.\\\n",
    "Developed completely in Python in the file \"snake_environment.py\", this implementation follows the Gym protocol, defining:\n",
    "\n",
    "1. Step\n",
    "2. Reset\n",
    "3. Render, which use PyGame\n",
    "4. Close\n",
    "\n",
    "All others functions are private and only used inside the class, for the exception of \"get_possible_action\" which is useful for our purpose.\n",
    "\n",
    "One important thing is that we actually defined a maximum number of step inside the environment to prevent infinte loop driven by bad policies while training. This is a parameter for the __init__ with default value 1000."
   ],
   "id": "c5a5d07775a569af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "env = SnakeEnv(render_mode=\"human\")\n",
    "\n",
    "done, keep = False, True\n",
    "\n",
    "state, _ = env.reset()\n",
    "action = 0\n",
    "\n",
    "while not done and keep:\n",
    "    action = random.choice(env.get_possible_actions(action))\n",
    "    state, reward, done, trunc, inf = env.step(action)\n",
    "    keep = env.render()\n",
    "\n",
    "env.close()"
   ],
   "id": "817161e222fff38b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **The Dimensionality Problem**\n",
    "\n",
    "Once the environment is defined, one can think about how big the space of all possible configuration is.\\\n",
    "Well, doing some (pretty bad but although valid) approximation, considering a state as the matrix representation of the grid with 0 (empty cell), 1 (apple), 2(head) and 3 (tail), the dimension of all possible configuration ends up being something like this:\n",
    "$$\n",
    "    |S| = (n\\times m)(n\\times m)2^{(n\\times m)}\n",
    "$$\n",
    "This should describe all the possible positions of the apple, all the possible position of the head and all possible configuration on the grid of the tails (now this is the big approximation, since the tail configuration is not independent from the head position). Anyway, even if this is an approximation one can simply add the \"blocks\" on the grid world (static cells that kill, if touched, the snake) and the dimension should exactly being that big.\n",
    "\n",
    "Now this is not a simple thing to deal with while learning. Solution? Binning (or bracketing, how we actually call it). Now on this soon."
   ],
   "id": "a8b624aa4d082126"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PART 2\n",
    "---\n",
    "*Algorithms*"
   ],
   "id": "a8940b47dbaa3e8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **BINNING (or Bracketing)**",
   "id": "21ec1f7cbeb8d4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As shown before the Snake game has a huge state dimension.\\\n",
    "Since a look up table of that dimension has no logic to exist (I don't even think our computers can store something like that) and since it is pretty impossible just to see one example for each pair state-action in an entire life time, something had to come in mind.\\\n",
    "Thanks to a lesson we learnt about binning (that we called bracketing for the entire project, shame on Francesco) and decided to try it out.\\\n",
    "Binning essentially is just a supervised technique (it is the human that codifies how it works) that agglomerates similar states together, in order to reduce the dimensionality of the problem.\\\n",
    "A very stupid example could be the following: each state just randomly be labelled as 0 or 1. And the agents now will not see the entire state representation, but only the label you gave it. Now this example is stupid because, using this random strategy, you end up with no knowledge at all. BUT, look at what happened at your state dimension: it fell down from whatever it was to 2! Pretty neat, uh?\n",
    "\n",
    "We discussed together end decided to try a lot of different binning techniques, and we end up discovering the big tradeoff in this field: **information against dimension**.\\\n",
    "Minimal state binning are easy to implement, but if they are too small it is not ensured that they will bring enough information to actually learn to the agent. On the other hand, if you give too many information to the agent, you will end up again with a too big state dimension to deal with.\n",
    "\n",
    "Another important aspect of binning is that you actually \"completely\" lose the transition function below your system. It is true that you can develop a new transition function on a new space, the \"binning space\", but it is not easy and it is not ensured to be relevant.\n",
    "\n",
    "Now we will quickly see all the binnings we implemented. We defined in the states_bracket.py a super class (StateBracket) which implement the protocol for all the binning techniques."
   ],
   "id": "3617b51c376f6feb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **FOOD INFORMATION ONLY**",
   "id": "477ed10e74a7a5c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our first approach was to give only information about the position of the apple wrt the head of the snake. Two variants of this idea came up:\n",
    "\n",
    "1. Food Relative Position \n",
    "2. Food Direction\n",
    "\n",
    "Both of the techniques are pretty straightforward to implement, and their computation is very fast as well. However, the snake loses information about its tail, the walls and the boundaries of the cell.\n",
    "\n",
    "\n",
    "##### **Food Relative Position**\n",
    "\n",
    "Once you get the apple position $(a_y, a_x)$ and the head position $(h_y, h_x)$ you just return as state the tuple $(a_y - h_y, a_x - h_x)$. This approach reduces the states' dimension up to $2m \\times 2n$\n",
    "\n",
    "##### **Food Direction**\n",
    "\n",
    "Once you get the apple position $(a_y, a_x)$ and the head position $(h_y, h_x)$ you just return as state the tuple $(a_y < h_y, a_y > h_x, a_x < h_x, a_x > h_x)$.\\\n",
    "Now, while the first one is straightforward, this a little more subtle. This tells you whether your head is above, below, to the left, or to the right of the food. \\\n",
    "This is a very minimal information: the states are condensed into just $8$ bins.\n",
    "\n"
   ],
   "id": "68bcd4ea63fa6cfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "state = np.array([\n",
    "    [0, 0, 0], \n",
    "    [0, 0, 1], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "frp = FoodRelativePositionBracket()\n",
    "fd = FoodDirectionBracket()\n",
    "print(frp.bracket(state))\n",
    "print(fd.bracket(state))"
   ],
   "id": "108a8b27963c1a64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "These binning techniques combines the things we have just seen plus information relative to the neighborhood of the head.\\\n",
    "Neighborhood could be both of Von Neumann ('V') type or Moore ('M') type.\n",
    "A Von Neumann neighborhood is defined as the four cells that are vertically or horizontally adjacent to the head of the snake.\n",
    "On the other hand, a Moore neighborhood considers vertically, horizontally and diagonally adjacent cells.\n",
    "\n",
    "The radius is a parameter for this binning. Notice that the greater the radius, the greater the total state dimension. The information within the neighborhood are expressed in the form of 0, 1, 2 and 3. 0 if a cell is free, 1 if a cell is occupied by the food, 2 if is occupied by the head and 3 if is occupied by the tail."
   ],
   "id": "26e36cfa3f946f60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "state = np.array([\n",
    "    [0, 0, 0], \n",
    "    [0, 0, 1], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "npfrp = NeighPlusFoodRelativePositionBracket(neigh='V', radius=1)\n",
    "npfd = NeighPlusFoodDirectionBracket(radius=1)\n",
    "print(npfrp.bracket(state))\n",
    "print(npfd.bracket(state))"
   ],
   "id": "f6b9640abe822bdc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **ONLY NEIGHBORHOOD INFORMATION**",
   "id": "7e90f951d7a5f70c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "An interesting experiment was the definition of the binning containing only information relative to the neighborhood of the head (adding the 2 value for the apple in this case).\\\n",
    "The idea was that, using this type of binning, the agent could have learnt to search being really careful, but probably with no knowledge on its own position it is impossible for it to learn a valid strategy."
   ],
   "id": "7e6ec4a93bd73d92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "state = np.array([\n",
    "    [0, 0, 0], \n",
    "    [0, 0, 1], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "n = NeighborhoodBracket()\n",
    "print(n.bracket(state))"
   ],
   "id": "bc272c9f6b931cd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **FOOD AND NEIGHBORHOOD AND TAIL INFORMATION**",
   "id": "32745fff2fe654ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The last type of binning explored combines all the informations described until now plus a little information about the tail.\\\n",
    "The relative information about the tail is its length, and should help the agent learn to be a little more careful when the tail gets longer."
   ],
   "id": "5d2ac59805ae8599"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "state = np.array([\n",
    "    [0, 0, 0], \n",
    "    [0, 0, 1], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "tpnpfrp = NeighPlusFoodRelativePositionPlusTailBracket()\n",
    "tpnpfd = NeighPlusFoodDirectionPlusTailBracket()\n",
    "print(tpnpfrp.bracket(state))\n",
    "print(tpnpfd.bracket(state))"
   ],
   "id": "c4566eae23b60b27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ALGORITHMS",
   "id": "71797dc42244ba39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Why not MDP**",
   "id": "e52247ba803bd388"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "How we have seen in the latest section, the only way to obtain a dealable state dimension is using the binning. \\\n",
    "The problem is that, using binning, we lose the transition function below our MDP. Indeed, being in the same state, taking the same action, ending into the same state, could possibly lead to different rewards.\n",
    "To give an example, consider the two situations below, where we marked with a $O$ the head of the snake, with $0$ its body and with $X$ the food (consider the snake without tail):\n",
    "\n",
    "\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " & 0 & 0 \\\\\n",
    "\\hline\n",
    " & O & 0 \\\\\n",
    "\\hline\n",
    "X &  &  \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " &  &  \\\\\n",
    "\\hline\n",
    " & O & 0 \\\\\n",
    "\\hline\n",
    "X &  & 0 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "Such situations are binned into the same condensed-state $(-1, -1)$. However, the action \"UP\" will lead in the first example to a negative reward because of the hit of the snake's tail, while  in the second case, to a zero reward.\n",
    "\n",
    "This erases the capability to retrieve good results solving the problem as an MDP via policy iteration.\n",
    "\n",
    "TO ARGUMENT A BIT MORE\n"
   ],
   "id": "1ad610d32db6b503"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Our Choices**",
   "id": "37c81a7f91a0d5aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "In the end we decided to develop 5 different **Model-free algorithms** (with also some variants of them) in order to take familiarity with the whole RL framework. These are the algorithms we implemented:\n",
    "\n",
    "1. Montecarlo \n",
    "2. SARSA\n",
    "3. QLearning\n",
    "4. DDQL\n",
    "5. Policy Gradient\n",
    "\n",
    "We firstly implemented a super class that defines a protocol for all the algorithms and provides useful function such as \"get_action_epsilon_greedy\" and so on.\\\n",
    "Useful method of the class are the save and upload methods, which can let to store the results obtained as a pickle dictionary in order to retrieve it later.\\\n",
    "In addiction the utils.py contains a lot of useful function used to deal with the default_dict, a structure we used to store the QValues look up table (for the algorithms that require it). Since we used a lot of binning we used default dict to possibly deal with no-fixed state dimension.\n",
    "\n",
    "Before diving in our actual implementation, let's briefly remind some key concepts that will be used.\n",
    "\n",
    "*Credit for the key concepts to [this tutorial](https://medium.com/@hsinhungw/intro-to-reinforcement-learning-monte-carlo-to-policy-gradient-1c7ede4eed6e)*"
   ],
   "id": "297a738ef59f9a9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Policy-based vs Value-based**\n",
    "\n",
    "Most of our algorithms are value-based but policy gradient is in fact policy-based. Let's define the difference between these two classes.\n",
    "\n",
    "1. **Policy-based methods**: The agent learns the policy directly.\n",
    "2. **Value-based methods**: The agent learns a value function that gives the expected return of being in a given state or performing a given action in a given state. The policy can then be derived from the learned value function."
   ],
   "id": "2a05f54305a18c5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Off-policy / On-policy**\n",
    "\n",
    "In RL, the agent generates experience by interacting with the environment under a certain policy and then learns the policy from those experiences. Given this in mind, algorithms can use different approaches to the policy. Two main classes exist, let's break them down.\n",
    "\n",
    "\n",
    "1. **On-policy methods**: The agent attempts to learn the policy that is also used to generate the experience.\n",
    "2. **Off-policy methods**: The agent learns a policy that is different from the one used to generate experiences."
   ],
   "id": "7a3705c422f85849"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Q-Value Learning**\n",
   "id": "c8d8e80422b412f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our goal is to find the optimal policy given only the experience of an environment. Again, the experience consists in the trajectories we perform as we explore the environment\n",
    "\n",
    "$\\tau_i = (S^i_0, A^i_0, R^i_1, S^i_1, A^i_1, R^i_1, \\dots S^i_{T^i})$\n",
    "\n",
    "\n",
    "To do this, it will be more convenient to consider, instead of the _state_-function $V_{\\pi} (s) $, the _state/action_-function $Q_{\\pi}(s,a)$. It is defined as:\n",
    "\n",
    "$$\n",
    "Q_{\\pi}(s,a) = \\mathbb{E}_\\pi\\bigg[ \\sum_{t=0}^\\infty \\gamma^t \\, R_{t+1} \\, \\, \\Big| \\, \\, S_0 = s, A_0 = a \\bigg] \\ .\n",
    "$$\n",
    "\n",
    "**Definition 1** _\"The expectation discounted cumulative sum of all future rewards when starting from state $s$, acting with action $a$ and then always following the policy $\\pi$\"_\n",
    "\n",
    "or equivalently as:\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) = \\mathbb{E}_{\\pi}\\bigg[ R_1 + \\gamma V_\\pi(S_1) \\,\\, \\Big| \\,\\, S_0 = s, A_0 = a\\bigg] \\ .\n",
    "$$\n",
    "\n",
    "**Definition 2** _\"The expectation value of the immediate reward plus the discounted $V$-value of the following state $S_1$, when starting from state $s$, acting with action $a$ and then always following the policy $\\pi$.\"_\n",
    "\n",
    "or again as:\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a) = \\mathbb{E}_{\\pi}\\bigg[ R_1 + \\gamma  \\, \\sum_{a'} \\, Q_\\pi(S_1, a') \\, \\pi(a' | S_1) \\,\\, \\Big| \\,\\, S_0 = s, A_0 = a\\bigg] \\ .\n",
    "$$\n",
    "**Definition 3** _\"The expectation value of the immediate reward plus the discounted $Q$-value of the following state $S_1$ and all possible actions $a'$ weighted by the probability of taking that action ($\\pi(a' | S_1)$), when starting from state $s$, acting with action $a$ and then always following the policy $\\pi$.\"_\n",
    "\n",
    "*Credit to the 4th tutorial for the definition*"
   ],
   "id": "51d0d8b11d038288"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Montecarlo**\n",
   "id": "caf07ce9ff3616f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Monte Carlo methods are ways of estimating value functions through experiences (sample sequence of states, actions, and rewards). Recall that the value of a state is the expected return (expected cumulative future discounted reward) starting from that state. One way to estimate the expected return is simply to average the returns observed after visits to that state. As more returns are observed, by the law of large numbers, the average should converge to the expected return. This idea underlies all Monte Carlo methods.\n",
    "\n",
    "Our first algorithm developed was the **on-policy Montecarlo**. It works as follows:\n",
    "\n",
    "1. On-policy MC uses an **ε-soft policy**. A soft policy refers to any policy that has a small, but finite, probability of selecting any possible action, ensuring exploration of alternative actions.\n",
    "2. After each episode, the **observed returns** are used to learn the q-value function, and then the policy is improved based on the learned value function for all the states visited in the episode.\n",
    "\n",
    "This means that we use the first definition. This definition has no bias but an high variance.\n",
    "\n",
    "![onpolicymc](./images/on_policy_mc_correct.png)"
   ],
   "id": "ea81ab41f47ad5a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=100)\n",
    "mc = Montecarlo(action_space=4, gamma=0.90, lr_v=0.01)\n",
    "eps = ConstantEpsilonDecay(0.3)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "mc.learning(env = env, epsilon_schedule= eps, n_episodes=400, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "mc.play(env=env, bracketer=bracketer)"
   ],
   "id": "1ac6731a5a12d91b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **SARSA**",
   "id": "62668643763437e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "SARSA is a fascinating on-policy method which use state action and reward (State Action Reward State Action) in order to achieve its goals. There are many variants of SARSA, and all of them are different in terms of how they update the Q-Value table. ```(ENRICO) Quali versioni? -> Expected Sarsa, Sarsa, lambda Sarsa? Forse sono solo expected sarsa e sarsa```\n",
    "\n",
    "We implemented both a SARSA(0) version, where the Q-values are updated each step, and a SARSA(λ). \\\n",
    "The second option, despite being a bit slower, is a more general and flexible algorithm which leaves the possibility to see more than one action at the time, so to have a more accurate update of the Q-values. In this version of SARSA we implemented a \"backward approach\" thanks to eligibility traces.\n",
    "\n",
    "The (common) pseudocode for SARSA(0) is here below:\n",
    "\n",
    "![sarsa](./images/sarsa.png)"
   ],
   "id": "693be9de27e12142"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=100)\n",
    "sarsa = SARSA(action_space=4, gamma=0.90, lr_v=0.01)\n",
    "eps = ConstantEpsilonDecay(0.3)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "sarsa.learning(env = env, epsilon_schedule= eps, n_episodes=500, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "sarsa.play(env=env, bracketer=bracketer)"
   ],
   "id": "3915e77f85cbafa8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=100)\n",
    "sarsal = SARSALambda(action_space=4, gamma=0.90, lr_v=0.01, lambda_value=0.1)\n",
    "eps = ConstantEpsilonDecay(0.3)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "sarsal.learning(env = env, epsilon_schedule= eps, n_episodes=500, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "sarsal.play(env=env, bracketer=bracketer)"
   ],
   "id": "c47190adcedcc75d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **QLearning**\n",
    "\n",
    "QLearning is the first off-policy algorithm we see. "
   ],
   "id": "4191100147e1d909"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=1000)\n",
    "ql = QLearning(action_space=4, gamma=0.90, lr_v=0.01)\n",
    "eps = LinearEpsilonDecay(1, 0.999, 0.2)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "ql.learning(env = env, epsilon_schedule= eps, n_episodes=20000, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "ql.play(env=env, bracketer=bracketer)"
   ],
   "id": "597c6b34262d2c6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ql.play(env=env, bracketer=bracketer)",
   "id": "b74fe1a704954b2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Double Deep Q Learning (DDQL)**",
   "id": "5298bed3fe2e31f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This algorithm is a variant of the Q-Learning algorithm, which is an off-policy method.\n",
    "\n",
    "It is basically a combination of Q-Learning and Deep Q-Networks (DQN), used to estimate the Q-values.\n",
    "The main idea behind DDQL is to use two separate neural networks for the estimation, which helps to reduce the overestimation bias that can occur in Deep Q-learning and in Q-learning in general.\n",
    "Indeed, in common Q-Learning, the use of the $\\max$ operator may lead to inflated Q-values. This consequently leads to suboptimal policies, especially in complex environments.\n",
    "\n",
    "DDQL addresses the problem by exploiting a **Online neural network** for choosing the best action given a certain state, and using a **Target neural network** for computing the estimated Q-values given the couple (state, action).\n",
    "The target network is updated less frequently than the online one, so the two are basically decoupled, and their predictions are independent. This helps to stabilize the learning process and reduce the overestimation bias.\n",
    "\n",
    "\\begin{ENRICO}\n",
    "Ricordati di parlare di replay buffer\n",
    "\\end{ENRICO}\n",
    "\n",
    "![ddql](./images/ddql.png)"
   ],
   "id": "76efa08f9109136b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "env = SnakeEnv(render_mode=\"non-human\", max_step=100)\n",
    "ddql = DeepDoubleQLearning(\n",
    "    action_space=4,\n",
    "    state_dim=bracketer.get_state_dim(),\n",
    "    gamma=0.90,\n",
    "    lr_v=0.001,\n",
    "    batch_size=128,\n",
    "    memory_size=10000,\n",
    "    target_update_freq=200,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "eps = ConstantEpsilonDecay(0.3)\n",
    "bracketer = NeighPlusFoodDirectionBracket()\n",
    "ddql.learning(env = env, epsilon_schedule= eps, n_episodes=2000, bracketer=bracketer)\n",
    "env = SnakeEnv(render_mode=\"human\", max_step=2000)\n",
    "ddql.play(env=env, bracketer=bracketer)"
   ],
   "id": "11aa770a00baedea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Several DDQL architectures have been trained.\n",
    "For all of them, the following hyperparameters are kept fixed:\n",
    "- ε: we chose a linearly-decaying epsilon for the ε-greedy policy, starting from ε=1 to ε=0.05, with a slope of -0.999.\n",
    "- learning rate: constant at 0.001\n",
    "- memory size: the maximum size of the memory buffer, fixed to 10000.\n",
    "- batch size: the number of samples from the memory buffer, fixed to 128.\n",
    "- frequency of update of the target neural network: 200 step.\n",
    "\n",
    "These values for thw hyperparameters are chosen thanks to the first tries."
   ],
   "id": "2e7c6152529683c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

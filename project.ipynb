{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5b1edd",
   "metadata": {},
   "source": [
    "# Jupyter Snake\n",
    "\n",
    "---\n",
    "\n",
    "In this project our goal is to beat the game of snake, applying multiple RL techniques in order to teach an agent how to play the game.\\\n",
    "The project is divided in 3 parts:\n",
    "1. Developement of the Environment\n",
    "2. Implementation of the Algorithms\n",
    "3. Learning and evaluating phase of the Algorithms\n",
    "\n",
    "This whole project is developed as final project for the \"Reinforcement Learning\" course (2024-2025).\n",
    "\n",
    "Authors : *Bredariol Francesco, Savorgnan Enrico, Tic Ruben*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27a3a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import *\n",
    "from eligibility_traces import *\n",
    "from epsilon_scheduler import * \n",
    "from snake_environment import *\n",
    "from states_bracket import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32296b",
   "metadata": {},
   "source": [
    "## PART 1\n",
    "---\n",
    "*Environment*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e4414",
   "metadata": {},
   "source": [
    "##### **The Game**\n",
    "\n",
    "For who who doesn't know Snake is a not just a game but a genre of action video games.\\\n",
    "It was born in 1976 competitive arcade video game Blockade, where the goal was to survive longer than others players while collecting the most possible food.\\\n",
    "In this game you control the head of a snake on grid world and you aim to eat some food in order to become bigger. The big difficulty here is that if you hit your tail (this is the only common rule for all snake variant) you die.\\\n",
    "There are multiple version of the game and some of them are really weird (where teleportation can occour or some food can actually make you die).\\\n",
    "We took in account the most basic version, where:\n",
    "\n",
    "1. The world is a discrete grid world of size $n\\times m$\n",
    "2. There is always only one food (apple) on the grid world, until you eat it and it changes position\n",
    "3. There are no periodic boundary conditions, meaning that if you hit a wall you die\n",
    "\n",
    "The rest is just as described as in the introduction to the game.\\\n",
    "Little side note is that this version is inspired by the \"Snake Byte\" published by Sirius Software in 1982.\n",
    "\n",
    "##### **The Implementation**\n",
    "\n",
    "Thanks to Gymnasium and PyGame the implementation of this simple version of the game is pretty straightforward.\\\n",
    "Developed completely in Python in the file \"snake_environment.py\", this implementation follows the Gym protocol, defining:\n",
    "\n",
    "1. Step\n",
    "2. Reset\n",
    "3. Render, which use PyGame\n",
    "4. Close\n",
    "\n",
    "All others functions are private and only used inside the class, for the exception of \"get_possible_action\" which is useful for our purpose.\n",
    "\n",
    "One important thing is that we actually defined a maximum number of step inside the environment to prevent infinte loop driven by bad policies while training. This is a parameter for the __init__ with default value 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ecb7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "env = SnakeEnv(render_mode=\"human\")\n",
    "\n",
    "done, keep = False, True\n",
    "\n",
    "state, _ = env.reset()\n",
    "action = 0\n",
    "\n",
    "while not done and keep:\n",
    "    action = random.choice(env.get_possible_actions(action))\n",
    "    state, reward, done, trunc, inf = env.step(action)\n",
    "    keep = env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413723b9",
   "metadata": {},
   "source": [
    "##### **The Dimensionality Problem**\n",
    "\n",
    "Once the environment is defined, one can think about how big the space of all possible configuration is.\\\n",
    "Well, doing some (pretty bad but although valid) approximation, considering a state as the matrix representation of the grid with 0 (empty cell), 1 (apple), 2(head) and 3 (tail), the dimension of all possible configuration ends up being something like this:\n",
    "$$\n",
    "    |S| = (n\\times m)(n\\times m)2^{(n\\times m)}\n",
    "$$\n",
    "This should describe all the possible positions of the apple, all the possible position of the head and all possible configuration on the grid of the tails (now this is the big approximation, since the tail configuration is not indepent from the head position). Anyway, even if this is an approximation one can simply add the \"blocks\" on the grid world (static cells that kill, if touched, the snake) and the dimension should exactly being that big.\n",
    "\n",
    "Now this is not a simple thing to deal with while learning. Solution? Bidding (or bracketing, how we actually call it). Now on this soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858975fc",
   "metadata": {},
   "source": [
    "## PART 2\n",
    "---\n",
    "*Algorithms*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea080f",
   "metadata": {},
   "source": [
    "##### **Why not MDP**\n",
    "\n",
    "-> Explaining why we do not use MDP. Lack of the transition function, too many state to actually defined a good Markov decision processes.\n",
    "\n",
    "##### **Our Choices**\n",
    "\n",
    "In the end we decided to develope 5 differents algorithms (with their variants) in order to take familiarity with the whole RL framework. These are the algorithms we implemented:\n",
    "\n",
    "1. Montecarlo \n",
    "2. SARSA\n",
    "3. QLearning\n",
    "4. DQL\n",
    "5. Policy Gradient\n",
    "\n",
    "We firstly implemented a super class that defines a protocol for all the algorithms and provides useful function such as \"get_action_epsilon_greedy\" and so on.\\\n",
    "In addiction the utils.py contains a lot of useful function used to deal with the default_dict, a structure we used to store the QValues look up table (for the algorithms that require it). Since we used a lot of bidding (more about this soon) we used default dict to possibly deal with no-fixed total dimension of all possible configurations.\n",
    "\n",
    "##### **Bidding (or Bracketing)**\n",
    "\n",
    "As shown before the Snake game has a huge state dimension.\\\n",
    "Since a look up table of that dimension has no logic to exist (I don't even think our computers can store something like that) and since it is pretty impossible just to see one example for each pair state-action in an entire life time, something had to come in mind.\\\n",
    "Thanks to a lesson we learnt about bidding (that we called bracketing for the entire project, shame on Francesco) and decided to try it out.\\\n",
    "Bidding essentialy is just a supervised techniques (since it is the human that codify how it works) that agglomerate similar states together, in order to reduce the dimensionality of the problem.\\\n",
    "A very stupid example could be the following: each state just randomly be labelled as 0 or 1. And the agents now will not see the entire state representation, but only the label you gave it. Now this example is stupid because, using this random strategy, you end up with no knowledge at all. BUT, loko at what happened at your state dimension: it falled down from whatever it was to 2! Pretty neat, uh?\n",
    "\n",
    "We discussed together end decided to try a lot of different bidding techniques, and we end up discovering the big tradeoff in this field: information against dimension.\\\n",
    "Minimal state bidding are easy to develop, but if they are too small it is not ensured that they will bring enough information to actually learn to the agent. On the other hand, if you give too many information to the agent, you will end up again with a too big state dimension to deal with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
